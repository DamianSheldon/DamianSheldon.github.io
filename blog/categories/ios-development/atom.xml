<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ios development | Hello World]]></title>
  <link href="http://DamianSheldon.github.io/blog/categories/ios-development/atom.xml" rel="self"/>
  <link href="http://DamianSheldon.github.io/"/>
  <updated>2017-04-10T08:52:10+08:00</updated>
  <id>http://DamianSheldon.github.io/</id>
  <author>
    <name><![CDATA[Sheldon]]></name>
    <email><![CDATA[dongmeilianghy@sina.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AVFoundation 使用笔记]]></title>
    <link href="http://DamianSheldon.github.io/blog/notes-of-using-avfoundation.html"/>
    <updated>2017-04-06T15:09:30+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/notes-of-using-avfoundation</id>
    <content type="html"><![CDATA[<p>使用一个框架时，我们可能有这么三个问题：</p>

<ol>
<li>这个框架是做什么的？</li>
<li>为什么要使用这个框架而不是其他的框架？</li>
<li>怎么用这个框架？</li>
</ol>


<h3>这个框架是做什么的？</h3>

<p>Apple 在 iOS Technology Overview 中的 Audio Technologies 和 Video Technologies 分别是这么介绍 AVFoundation 的：</p>

<blockquote><p>AV Foundation is an Objective-C interface for managing the recording and playback of audio and video. Use this framework for recording audio and when you need fine-grained control over the audio playback process.</p>

<p>AV Foundation provides advanced video playback and recording capabilities. Use this framework in situations where you need more control over the presentation or recording of video. For example, augmented reality apps could use this framework to layer live video content with other app-provided content.</p></blockquote>

<p>从这两个介绍中我们可以知道 AVFoundation 是用来播放和录制音频和视频的。</p>

<p>在 AVFoundation Programming Guide 中则是这么介绍的：</p>

<blockquote><p>AVFoundation is one of several frameworks that you can use to play and create time-based audiovisual media. It provides an Objective-C interface you use to work on a detailed level with time-based audiovisual data. For example, you can use it to examine, create, edit, or reencode media files. You can also get input streams from devices and manipulate video during realtime capture and playback.</p></blockquote>

<p>从这里我们可以知道它不仅可以播放和创建基于时间的视听媒体，还可以让我们在很细微的层面去操作这些视听数据。例如，你可以使用它检查、创建、编辑或者重编码媒体文件。你还可以用它从设备那里拿到输出流，并且可以在实时的捕获和播放过程中操作视频。</p>

<p>所以结论就是：这个框架是处理音频和视频的，而且处理的粒度可以非常细。</p>

<!--more-->


<h3>为什么要用这个框架而不是其他的框架？</h3>

<p>在选择框架时我们的原则应该首先是使用 Apple 自己提供的框架，其次才是第三方框架。在 Apple 自带的框架中选择时，又应该按抽象程度从高到低去选择。在音频技术中，抽象程度是这样的：Media Player framework > AVFoundation > OpenAL > Core Audio; 在视频技术中：UIImagePickerController > AVKit > AVFoundation > Core Media.</p>

<p>在音视频技术中，抽象程度高于 AVFoundation 的技术多侧重于简单的播放和录制，要进行其他的操作时则要使用 AVFoundation，而且它的能力也比较强，所以通常要对媒体数据进行处理时，我们会经常使用到它，它不满足要求时才去寻找其他的技术。</p>

<h3>怎么用这个框架？</h3>

<p>前面提到 AVFoundation 是用来播放、录制和操作视听数据的，操作视听数据则可以细分为 Editing 和 Exporting，所以我们这里会介绍这个框架:Playback, Capture, Editing 和 Exporting 四个大方面的使用。</p>

<h4>Playback</h4>

<p>在介绍怎么使用 AVFoundation 播放视听媒体之前，我们还要聊聊在 AVFoundation 中是怎么表示媒体的。AVFoundation 中用来代表媒体最主要的类是 AVAsset, 一个 AVAsset 实例是一片或多片媒体数据(音频曲目和视频轨迹)集合的综合代表。它提供关于这个集合的信息，例如它的标题，持续时间，本身的展示尺寸等等。AVAsset 没有绑定特定的数据类型。它是其他用来从指定 URL 媒体创建资产实例和创建新构成的父类。</p>

<p>资产中每片单独的媒体数据是统一的类型称为track. 在经典的简单场景，一个轨迹代表音频组件，另一个轨迹代表视频组件；在一个复杂的构成中，这里可能有多个重叠的音视频轨迹。</p>

<p>你为播放配置资产的方法取决于你想要播放资产的种类，广义上来讲，这里有两大类型：基于文件的资产和基于流的资产。</p>

<ol>
<li><p>加载和播放基于文件的资产。</p>

<ul>
<li>Create an asset using AVURLAsset.</li>
<li>Create an instance of AVPlayerItem using the asset.</li>
<li>Associate the item with an instance of AVPlayer.</li>
<li>Wait until the item’s status property indicates that it’s ready to play (typically you use key-value observing to receive a notification when the status changes).</li>
</ul>
</li>
<li><p>为播放创建和准备一个 HTTP 实时流。</p></li>
</ol>


<pre><code>NSURL *url = [NSURL URLWithString:@"&lt;#Live stream URL#&gt;];
// You may find a test stream at &lt;http://devimages.apple.com/iphone/samples/bipbop/bipbopall.m3u8&gt;.
self.playerItem = [AVPlayerItem playerItemWithURL:url];
[playerItem addObserver:self forKeyPath:@"status" options:0 context:&amp;ItemStatusContext];
self.player = [AVPlayer playerWithPlayerItem:playerItem];
</code></pre>

<ol>
<li><p>如果你不知道你拥有的 URL 是什么类型。</p>

<p> 1)Try to initialize an AVURLAsset using the URL, then load its tracks key.
 If the tracks load successfully, then you create a player item for the asset.</p>

<p> 2)If 1 fails, create an AVPlayerItem directly from the URL.
 Observe the player’s status property to determine whether it becomes playable.</p></li>
</ol>


<h4>Capture</h4>

<p>为了管理来自相机、麦克风的捕获，你组装对象去表示输入和输出，使用 AVCaptureSession 的实例来协调它们之间的数据流。你最少需要：</p>

<ul>
<li>一个 AVCaptureDevice 的实例来表示输入设备，例如相机或麦克风</li>
<li>一个 AVCaptureInput 具体子类的实例去配置来自输入设备的端口</li>
<li>一个 AVCaptureOutput 具体子类的实例去管理到电影或静态图片的输出</li>
<li>一个 AVCaptureSession 的实例来协调从输入到输出的数据流</li>
</ul>


<p>Capturing Video Frames as UIImage Objects</p>

<pre><code>// 1. Create and Configure a Capture Session
AVCaptureSession *session = [[AVCaptureSession alloc] init];
session.sessionPreset = AVCaptureSessionPresetMedium;

// 2. Create and Configure the Device and Device Input
AVCaptureDevice *device =
[AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];

NSError *error = nil;
AVCaptureDeviceInput *input =
[AVCaptureDeviceInput deviceInputWithDevice:device error:&amp;error];
if (!input) {
    // Handle the error appropriately.
}
[session addInput:input];

// 3. Create and Configure the Video Data Output
AVCaptureVideoDataOutput *output = [[AVCaptureVideoDataOutput alloc] init];
[session addOutput:output];
output.videoSettings =
@{ (NSString *)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) };
output.minFrameDuration = CMTimeMake(1, 15);

dispatch_queue_t queue = dispatch_queue_create("MyQueue", NULL);
[output setSampleBufferDelegate:self queue:queue];
dispatch_release(queue);

// 4. Implement the Sample Buffer Delegate Method
- (void)captureOutput:(AVCaptureOutput *)captureOutput
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
fromConnection:(AVCaptureConnection *)connection {

    UIImage *image = imageFromSampleBuffer(sampleBuffer);
    // Add your code here that uses the image.
}
// 5. Starting and Stopping Recording
NSString *mediaType = AVMediaTypeVideo;

[AVCaptureDevice requestAccessForMediaType:mediaType completionHandler:^(BOOL granted) {
    if (granted)
    {
        //Granted access to mediaType
        [self setDeviceAuthorized:YES];
    }
    else
    {
        //Not granted access to mediaType
        dispatch_async(dispatch_get_main_queue(), ^{
                [[[UIAlertView alloc] initWithTitle:@"AVCam!"
                message:@"AVCam doesn't have permission to use Camera, please change privacy settings"
                delegate:self
                cancelButtonTitle:@"OK"
                otherButtonTitles:nil] show];
                [self setDeviceAuthorized:NO];
                });
    }
}];

[session startRunning];

// To stop recording, you send the session a stopRunning message.
</code></pre>

<h4>Editing</h4>

<p>AVFoundation 框架提供了丰富的类来方便编辑音视资产。编辑 API 的核心是 composition. 一个 Compostion 是简单的来自一个或多个不同媒体资产的聚合。AVMutableCompostion 类提供插入和移除轨迹的接口，并且管理它们的时间顺序。图 3-1 展示了一个新的 composition
是如何用由现存资产联合形成的新资产拼装在一起的。如果你所有想要做的就是将多个资产按顺序的合成到一个单一的文件，那么这就是你得知道的所有细节。如果你想对你 compostion 里面的轨迹进行自定义的音频或视频处理，你相应地需要引入一个 audio mix 或 video compostion.</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableComposition" src="http://DamianSheldon.github.io/images/avmutablecomposition_2x.png">
</div>


<p>使用 AVMutableAudioMix 类， 你可以在你的 composition 的音频轨迹上进行自定义音频处理，像图 3-2 显示的。你现在可以指定一个最大的音量或者设置 volume ramp.</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableAudioMix" src="http://DamianSheldon.github.io/images/avmutableaudiomix_2x.png">
</div>


<p>你为了编辑可以像图 3-3 那样使用 AVMutableVideoCompostion 类来直接操作你 compostion 里的视频轨迹. 拥有一个 video composition, 你可以为输出视频指定想要的渲染尺寸,缩放以及帧率。通过一个 video composition&rsquo;s instruction(由 AVMutableVideoCompositionInstruction 类代表)，你可以修改你视频的背景颜色和应用 layer instructions. 这些 layer instructions（由 AVMutableVideoCompositionLayerInstruction 类代表) 可以用来应用 transforms, transform ramps, opacity and opacity ramps。Video
composition 类使用 animationTool 属性赋予你引入来自 Core Animation 框架效果的能力。</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableVideoCompostion" src="http://DamianSheldon.github.io/images/avmutablevideocomposition_2x.png">
</div>


<p>为了把你的 compostion 和 audio mix, video compostion 混合，你使用一个 AVAssetExportSession 对象，像 图 3-4 那样。你用你的 compostion 初始化 export session, 然后简单地把你的 audio mix 和 video composition 相应地赋值给 audioMix 和 videoComposition 属性。</p>

<div style="text-align:center" markdown="1">
    <img name="AVAssetExportSession" src="http://DamianSheldon.github.io/images/puttingitalltogether_2x.png">
</div>


<p>Combining Multiple Assets and Saving the Result to the Camera Roll</p>

<pre><code>// 1. Creating the Composition
AVMutableComposition *mutableComposition = [AVMutableComposition composition];
AVMutableCompositionTrack *videoCompositionTrack = [mutableComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
AVMutableCompositionTrack *audioCompositionTrack = [mutableComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];

// 2. Adding the Assets
AVAssetTrack *firstVideoAssetTrack = [[firstVideoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
AVAssetTrack *secondVideoAssetTrack = [[secondVideoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration) ofTrack:firstVideoAssetTrack atTime:kCMTimeZero error:nil];
[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, secondVideoAssetTrack.timeRange.duration) ofTrack:secondVideoAssetTrack atTime:firstVideoAssetTrack.timeRange.duration error:nil];
[audioCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration)) ofTrack:[[audioAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0] atTime:kCMTimeZero error:nil];

// 3. Checking the Video Orientations
BOOL isFirstVideoPortrait = NO;
CGAffineTransform firstTransform = firstVideoAssetTrack.preferredTransform;
// Check the first video track's preferred transform to determine if it was recorded in portrait mode.
if (firstTransform.a == 0 &amp;&amp; firstTransform.d == 0 &amp;&amp; (firstTransform.b == 1.0 || firstTransform.b == -1.0) &amp;&amp; (firstTransform.c == 1.0 || firstTransform.c == -1.0)) {
        isFirstVideoPortrait = YES;
}
BOOL isSecondVideoPortrait = NO;
CGAffineTransform secondTransform = secondVideoAssetTrack.preferredTransform;
// Check the second video track's preferred transform to determine if it was recorded in portrait mode.
if (secondTransform.a == 0 &amp;&amp; secondTransform.d == 0 &amp;&amp; (secondTransform.b == 1.0 || secondTransform.b == -1.0) &amp;&amp; (secondTransform.c == 1.0 || secondTransform.c == -1.0)) {
        isSecondVideoPortrait = YES;
}
if ((isFirstVideoAssetPortrait &amp;&amp; !isSecondVideoAssetPortrait) || (!isFirstVideoAssetPortrait &amp;&amp; isSecondVideoAssetPortrait)) {
        UIAlertView *incompatibleVideoOrientationAlert = [[UIAlertView alloc] initWithTitle:@"Error!" message:@"Cannot combine a video shot in portrait mode with a video shot in landscape mode." delegate:self cancelButtonTitle:@"Dismiss" otherButtonTitles:nil];
            [incompatibleVideoOrientationAlert show];
                return;
}

// 4. Applying the Video Composition Layer Instructions
AVMutableVideoCompositionInstruction *firstVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set the time range of the first instruction to span the duration of the first video track.
firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);
AVMutableVideoCompositionInstruction * secondVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set the time range of the second instruction to span the duration of the second video track.
secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));
AVMutableVideoCompositionLayerInstruction *firstVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];
// Set the transform of the first layer instruction to the preferred transform of the first video track.
[firstVideoLayerInstruction setTransform:firstTransform atTime:kCMTimeZero];
AVMutableVideoCompositionLayerInstruction *secondVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];
// Set the transform of the second layer instruction to the preferred transform of the second video track.
[secondVideoLayerInstruction setTransform:secondTransform atTime:firstVideoAssetTrack.timeRange.duration];
firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];
secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];
AVMutableVideoComposition *mutableVideoComposition = [AVMutableVideoComposition videoComposition];
mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];

// 5. Setting the Render Size and Frame Duration
CGSize naturalSizeFirst, naturalSizeSecond;
// If the first video asset was shot in portrait mode, then so was the second one if we made it here.
if (isFirstVideoAssetPortrait) {
    // Invert the width and height for the video tracks to ensure that they display properly.
        naturalSizeFirst = CGSizeMake(firstVideoAssetTrack.naturalSize.height, firstVideoAssetTrack.naturalSize.width);
            naturalSizeSecond = CGSizeMake(secondVideoAssetTrack.naturalSize.height, secondVideoAssetTrack.naturalSize.width);
}
else {
    // If the videos weren't shot in portrait mode, we can just use their natural sizes.
        naturalSizeFirst = firstVideoAssetTrack.naturalSize;
            naturalSizeSecond = secondVideoAssetTrack.naturalSize;
}
float renderWidth, renderHeight;
// Set the renderWidth and renderHeight to the max of the two videos widths and heights.
if (naturalSizeFirst.width &gt; naturalSizeSecond.width) {
        renderWidth = naturalSizeFirst.width;
}
else {
        renderWidth = naturalSizeSecond.width;
}
if (naturalSizeFirst.height &gt; naturalSizeSecond.height) {
        renderHeight = naturalSizeFirst.height;
}
else {
        renderHeight = naturalSizeSecond.height;
}
mutableVideoComposition.renderSize = CGSizeMake(renderWidth, renderHeight);
// Set the frame duration to an appropriate value (i.e. 30 frames per second for video).
mutableVideoComposition.frameDuration = CMTimeMake(1,30);

// 6. Exporting the Composition and Saving it to the Camera Roll
// Create a static date formatter so we only have to initialize it once.
static NSDateFormatter *kDateFormatter;
if (!kDateFormatter) {
        kDateFormatter = [[NSDateFormatter alloc] init];
            kDateFormatter.dateStyle = NSDateFormatterMediumStyle;
                kDateFormatter.timeStyle = NSDateFormatterShortStyle;
}
// Create the export session with the composition and set the preset to the highest quality.
AVAssetExportSession *exporter = [[AVAssetExportSession alloc] initWithAsset:mutableComposition presetName:AVAssetExportPresetHighestQuality];
// Set the desired output URL for the file created by the export process.
exporter.outputURL = [[[[NSFileManager defaultManager] URLForDirectory:NSDocumentDirectory inDomain:NSUserDomainMask appropriateForURL:nil create:@YES error:nil] URLByAppendingPathComponent:[kDateFormatter stringFromDate:[NSDate date]]] URLByAppendingPathExtension:CFBridgingRelease(UTTypeCopyPreferredTagWithClass((CFStringRef)AVFileTypeQuickTimeMovie, kUTTagClassFilenameExtension))];
// Set the output file type to be a QuickTime movie.
exporter.outputFileType = AVFileTypeQuickTimeMovie;
exporter.shouldOptimizeForNetworkUse = YES;
exporter.videoComposition = mutableVideoComposition;
// Asynchronously export the composition to a video file and save this file to the camera roll once export completes.
[exporter exportAsynchronouslyWithCompletionHandler:^{
        dispatch_async(dispatch_get_main_queue(), ^{
                    if (exporter.status == AVAssetExportSessionStatusCompleted) {
                                    ALAssetsLibrary *assetsLibrary = [[ALAssetsLibrary alloc] init];
                                                if ([assetsLibrary videoAtPathIsCompatibleWithSavedPhotosAlbum:exporter.outputURL]) {
                                                                    [assetsLibrary writeVideoAtPathToSavedPhotosAlbum:exporter.outputURL completionBlock:NULL];
                                                                                }
                                                                                        }
                                                                                            });
}];
</code></pre>

<h4>Exporting</h4>

<p>为了读写视听资产，你必须使用 AVFoundation 框架提供的导出 API. AVAssetExportSession 类为简单的导出需求提供接口，例如修改文件格式或者裁剪资产的长度。对于更深的导出需求，使用 AVAssetReader 和 AVAssetWriter 类。</p>

<p>当你想要操作资产的内容时使用 AVAssetReader. 例如，你可能读取资产中的音频轨迹去生成表示声波的图形。使用 AVAssetWriter 从像 sample buffers 或 still images 的媒体中生成资产。</p>

<p>Reading an Asset</p>

<p>每个 AVAssetReader 对象一次只能关联单个的资产， 但是这个资产可能包含多个轨迹。基于这个原因，为了配置如何去读取媒体数据，你必须在读取前给你的 asset reader 赋予 AVAssetReaderOutput 的具体子类。这里有三个具体的子类：AVAssetReaderTrackOutput, AVAssetReaderAudioMixOutput 和 AVAssetReaderVideoCompositionOutput.</p>

<ol>
<li>Creating the Asset Reader</li>
</ol>


<pre><code>NSError *outError;
AVAsset *someAsset = &lt;#AVAsset that you want to read#&gt;;
AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:someAsset error:&amp;outError];
BOOL success = (assetReader != nil);
</code></pre>

<ol>
<li>Setting Up the Asset Reader Outputs</li>
</ol>


<pre><code>AVAsset *localAsset = assetReader.asset;
// Get the audio track to read.
AVAssetTrack *audioTrack = [[localAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
// Decompression settings for Linear PCM
NSDictionary *decompressionAudioSettings = @{ AVFormatIDKey : [NSNumber numberWithUnsignedInt:kAudioFormatLinearPCM] };
// Create the output with the audio track and decompression settings.
AVAssetReaderOutput *trackOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:audioTrack outputSettings:decompressionAudioSettings];
// Add the output to the reader if possible.
if ([assetReader canAddOutput:trackOutput])
        [assetReader addOutput:trackOutput];
</code></pre>

<ol>
<li>Reading the Asset’s Media Data</li>
</ol>


<pre><code>// Start the asset reader up.
[self.assetReader startReading];
BOOL done = NO;
while (!done)
{
    // Copy the next sample buffer from the reader output.
    CMSampleBufferRef sampleBuffer = [self.assetReaderOutput copyNextSampleBuffer];
    if (sampleBuffer)
    {
        // Do something with sampleBuffer here.
        CFRelease(sampleBuffer);
        sampleBuffer = NULL;
    }
    else
    {
        // Find out why the asset reader output couldn't copy another sample buffer.
        if (self.assetReader.status == AVAssetReaderStatusFailed)
        {
            NSError *failureError = self.assetReader.error;
            // Handle the error here.
        }
        else
        {
            // The asset reader output has read all of its samples.
            done = YES;
        }
    }
}
</code></pre>

<p>Writing an Asset</p>

<p>AVAssetWriter 类将多个来源的媒体数据按指定的文件格式写出到单一的文件。你不需要将你的 asset writer 对象和指定的资产关联起来，但是你必须为你想要创建的输出文件使用一个 asset writer. 因为一个 asset writer 可以写出来自多个源的媒体数据，你必须为你想要写出到输出文件的单独轨迹创建一个 AVAssetWriterInput 对象。每个 AVAssetWriterInput 对象期望收到 CMSampleBufferRef 对象格式的数据，但是如果你想追加 CVPixelBufferRef 对象到你的 asset writer input, 使用 AVAssetWriterInputPixelBufferAdaptor 类。</p>

<pre><code>// 1. Creating the Asset Writer
NSError *outError;
NSURL *outputURL = &lt;#NSURL object representing the URL where you want to save the video#&gt;;
AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:outputURL
fileType:AVFileTypeQuickTimeMovie
error:&amp;outError];
BOOL success = (assetWriter != nil);

// 2. Setting Up the Asset Writer Inputs
// Configure the channel layout as stereo.
AudioChannelLayout stereoChannelLayout = {
    .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,
    .mChannelBitmap = 0,
    .mNumberChannelDescriptions = 0
};

// Convert the channel layout object to an NSData object.
NSData *channelLayoutAsData = [NSData dataWithBytes:&amp;stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];

// Get the compression settings for 128 kbps AAC.
NSDictionary *compressionAudioSettings = @{
AVFormatIDKey         : [NSNumber numberWithUnsignedInt:kAudioFormatMPEG4AAC],
                        AVEncoderBitRateKey   : [NSNumber numberWithInteger:128000],
                        AVSampleRateKey       : [NSNumber numberWithInteger:44100],
                        AVChannelLayoutKey    : channelLayoutAsData,
                        AVNumberOfChannelsKey : [NSNumber numberWithUnsignedInteger:2]
};

// Create the asset writer input with the compression settings and specify the media type as audio.
AVAssetWriterInput *assetWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio outputSettings:compressionAudioSettings];
// Add the input to the writer if possible.
if ([assetWriter canAddInput:assetWriterInput])
    [assetWriter addInput:assetWriterInput];

// 3. Writing Media Data
// Prepare the asset writer for writing.
    [self.assetWriter startWriting];
    // Start a sample-writing session.
    [self.assetWriter startSessionAtSourceTime:kCMTimeZero];
    // Specify the block to execute when the asset writer is ready for media data and the queue to call it on.
    [self.assetWriterInput requestMediaDataWhenReadyOnQueue:myInputSerialQueue usingBlock:^{
        while ([self.assetWriterInput isReadyForMoreMediaData])
        {
            // Get the next sample buffer.
            CMSampleBufferRef nextSampleBuffer = [self copyNextSampleBufferToWrite];
            if (nextSampleBuffer)
            {
                // If it exists, append the next sample buffer to the output file.
                [self.assetWriterInput appendSampleBuffer:nextSampleBuffer];
                CFRelease(nextSampleBuffer);
                nextSampleBuffer = nil;
            }
            else
            {
                // Assume that lack of a next sample buffer means the sample buffer source is out of samples and mark the input as finished.
                [self.assetWriterInput markAsFinished];
                break;
            }
        }
    }];
</code></pre>

<p>Reference:<br/>
iOS Technology Overview<br/>
AVFoundation Programming Guide</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Photos 框架的基本使用]]></title>
    <link href="http://DamianSheldon.github.io/blog/photos-framework-usage.html"/>
    <updated>2016-12-23T14:23:33+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/photos-framework-usage</id>
    <content type="html"><![CDATA[<p>从 iOS 9 开始 Apple 把 Asset Library 标记为废弃状态，并建议开发者使用 Photos 框架。</p>

<blockquote><p>The Assets Library framework is deprecated as of iOS 9.0. Instead, use the Photos framework instead, which in iOS 8.0 and later provides more features and better performance for working with a user’s photo library.</p></blockquote>

<p>不幸的是 Apple 并没有发布相关的使用指导文档，只有一个相关 Demo。使用的时候固然可以回头参考这个 Demo，但这样的效率不是很高，很多概念也容易忘记，所以这里做个简单的总结。</p>

<p>Photos 中有不少类，其中几个犹为关键。PHPhotoLibary 是我们操作 Photo Library 里面资源的入口对象，所有的操作都通过它完成。PHCollectionList 表示相册中的专题列表；PHAssetCollection 表示专题；PHAsset 表示资源，如 images, videos, and Live Photos.</p>

<p>我们基本的需求就是 CRUD, 这些操作是需要用户授权的，记得先获取权限再操作， 下面我们展示相关的代码片段。</p>

<h3>Create</h3>

<ol>
<li>创建一个资源</li>
</ol>


<pre><code>PHPhotoLibrary.shared().performChanges({
            PHAssetChangeRequest.creationRequestForAsset(from: image)
        }, completionHandler: {success, error in
            if !success { print("error creating asset: \(error)") }
        })
</code></pre>

<ol>
<li>创建一个资源到指定的专题</li>
</ol>


<pre><code>PHPhotoLibrary.shared().performChanges({
            let creationRequest = PHAssetChangeRequest.creationRequestForAsset(from: image)
            if let assetCollection = self.assetCollection {
            let addAssetRequest = PHAssetCollectionChangeRequest(for: assetCollection)
            addAssetRequest?.addAssets([creationRequest.placeholderForCreatedAsset!] as NSArray)
            }
        }, completionHandler: {success, error in
            if !success { print("error creating asset: \(error)") }
        })
</code></pre>

<!-- more -->


<h3>Read (Fetch)</h3>

<p>获取资源是通过 PHAsset 提供的一系列以 fetchXXX 开头的类方法，选择哪个方法取决于需求，这里示例其中两个我觉得常用的方法。</p>

<ol>
<li><code>class func fetchAssets(with options: PHFetchOptions?) -&gt; PHFetchResult&lt;PHAsset&gt;</code></li>
</ol>


<p>我们可以用这个方法获取 Photo Library 里面所有的资源。</p>

<pre><code>let allPhotosOptions = PHFetchOptions()
    allPhotosOptions.sortDescriptors = [NSSortDescriptor(key: "creationDate", ascending: true)]
self.fetchResult = PHAsset.fetchAssets(with: allPhotosOptions)
</code></pre>

<ol>
<li><code>class func fetchAssets(in assetCollection: PHAssetCollection, options: PHFetchOptions?) -&gt; PHFetchResult&lt;PHAsset&gt;</code></li>
</ol>


<p>我们可以用这个方法获取指定专题里面的资源。例如我们想获取 Camera Roll 这个专题里面的资源：</p>

<pre><code>let cameraRoll: PHFetchResult&lt;PHAssetCollection&gt; = PHAssetCollection.fetchAssetCollections(with: .smartAlbum, subtype: .smartAlbumUserLibrary, options: nil).firstObject
let fetchResult = PHAsset.fetchAssets(in: cameraRoll, options: nil)
</code></pre>

<h3>Update (Edit)</h3>

<p>编辑的基本的做法是先用资源请求一个 PHContentEditingInput，然后编辑资源，为了方便用户之后继续编辑或撤销可以实例化一个 PHAdjustmentData 对象来持有相关信息。编辑完成之后对于图片和视频需要实例化一个 PHContentEditingOutput 来完成输出，PHContentEditingOutput 的 adjustmentData 属性关联之前的 PHAdjustmentData, 并把编辑完成的内容输出到 PHContentEditingOutput 的 renderedContentURL。最后创建一个 PHAssetChangeRequest 对象，设置它的 contentEditingOutput 为
之前实例化的 PHContentEditingOutput。</p>

<p>这部分的代码会多点，具体可以查看 <a href="https://github.com/DamianSheldon/PhotosFrameworkUsage">Demo</a>.</p>

<h3>Delete</h3>

<pre><code>PHPhotoLibrary.shared().performChanges({ 
        PHAssetChangeRequest.deleteAssets([self.asset] as NSArray)
        }) { (success, error) in
    DispatchQueue.main.sync {
        self.trashButton.isEnabled = success ? false : true
    }

    if success {
        print("delete asset successfully")
    }
    else {
        print("can't delete asset: \(error)")
    }
}
</code></pre>

<h3>完整 Demo</h3>

<p><a href="https://github.com/DamianSheldon/PhotosFrameworkUsage">PhotosFrameworkUsage</a></p>

<h3>Caveat</h3>

<p>使用过程中遇到一个坑，这里记一下。</p>

<pre><code>guard let inputImage = CIImage(contentsOf: input.fullSizeImageURL!)
            else { fatalError("can't load input image to edit") }

// Apply the filter.
let outputImage = inputImage
    .applyingOrientation(input.fullSizeImageOrientation)
.applyingFilter(filterName, withInputParameters: nil)

// List 1.
let uiImage = UIImage(ciImage: outputImage)

// List 2.
if let cgImage = CIContext(options: nil).createCGImage(outputImage, from: outputImage.extent) {
    let uiImage = UIImage(cgImage:cgImage)
}
else {
    print("instance UIImage from CGImage failed!")    
}

// Ouput
if let data = UIImageJPEGRepresentation(uiImage, 0.7) {
    // NSData - (BOOL)writeToURL:(NSURL *)url atomically:(BOOL)atomically;
    do {
        try data.write(to: output.renderedContentURL)

    } catch let error {
        print("output filtered image to specify URL failed: \(error)")
    }
}
else {
    print("generate JPEG representation data failed")
        return
}
</code></pre>

<p>这里的问题是直接用 CIImage 实例化  UIImage 会失败，得转成 CGImage 然后实例化 UIImage. 至于它的原因我暂时还不清楚。</p>

<p>Reference:<a href="http://stackoverflow.com/questions/29732886/uiimagejpegrepresentation-returns-nil">UIImageJPEGRepresentation returns nil</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS 开发问题汇总(九)]]></title>
    <link href="http://DamianSheldon.github.io/blog/ios-development-problems-part-9.html"/>
    <updated>2016-11-15T14:49:17+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/ios-development-problems-part-9</id>
    <content type="html"><![CDATA[<h3>1.Curried functions in Swift</h3>

<p>A:There’s a difference between self.methodname (which you are using), and Classname.methodname.</p>

<p>The former, when called within a class’s method, will give you a function bound with that class instance. So if you call it, it will be called on that instance.</p>

<p>The latter gives you a curried function that takes as an argument any instance of Classname, and returns a new function that is bound to that instance. At this point, that function is like the first case (only you can bind it to any instance you like).</p>

<p>Here’s an example to try and show that a bit better:</p>

<pre><code>class C {
    private let _msg: String
        init(msg: String) { _msg = msg }

    func c_print() { print(_msg) }

    func getPrinter() -&gt; ()-&gt;() { return self.c_print }
}

let c = C(msg: "woo-hoo")
let f = c.getPrinter()
// f is of type (())-&gt;()
f() // prints "woo-hoo"

let d = C(msg: "way-hey")

let g = C.c_print
// g is of type (C) -&gt; (()) -&gt; (),
// you need to feed it a C:
g(c)() // prints "woo-hoo"
g(d)() // prints "way-hey"

// instead of calling immediately,
// you could store the return of g:
let h = g(c)
// at this point, f and h amount to the same thing:
// h is of type (())-&gt;()
h() // prints "woo-hoo"
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/27644702/curried-functions-in-swift">Curried functions in SWIFT</a></p>

<h3>2.NSLog on devices in iOS 10 / Xcode 8 will truncate.</h3>

<p>A:A temporary solution, just redefine all NSLOG to printf in a global header file.</p>

<pre><code>#define NSLog(FORMAT, ...) printf("%s\n", [[NSString stringWithFormat:FORMAT, ##__VA_ARGS__] UTF8String]);
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/39584707/nslog-on-devices-in-ios-10-xcode-8-seems-to-truncate-why">NSLog on devices in iOS 10 / Xcode 8 seems to truncate? Why？</a></p>

<!--more-->


<h3>3.UIImagePickerViewController is black doesn&rsquo;t work for selecting photo or taking picture.</h3>

<p>A:I instance it via <code>UIImagePickerViewController(nibName:nil, boundle:nil)</code> that doesn&rsquo;t work, and change to <code>UIImagePickerViewController()</code> it works.</p>

<h3>4.How to create dispatch queue in Swift?</h3>

<p>A:</p>

<pre><code>// Concurrent dispatch queue
let concurrentQueue = DispatchQueue(label: "queuename", attributes:.concurrent)

// Serial dispatch queue
let serialQueue = DispatchQueue(label: "queuename")
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/37805885/how-to-create-dispatch-queue-in-swift-3">How to create dispatch queue in Swift 3</a></p>

<h3>5.How to create a bitmap graphics context in Swift 3?</h3>

<p>A:</p>

<pre><code>// Create a bitmap graphics context with the sample buffer data
guard let context = CGContext(data: baseAddress, width: width, height: height, bitsPerComponent: 8,
        bytesPerRow: bytesPerRow, space: colorSpace, bitmapInfo: CGImageAlphaInfo.premultipliedFirst.rawValue | CGBitmapInfo.byteOrder32Little.rawValue) else {
    return nil
}
</code></pre>

<p>Reference: <a href="http://stackoverflow.com/questions/24109149/cgbitmapcontextcreate-error-with-swift">CGBitmapContextCreate error with swift</a></p>

<h3>6.Required plug-in compatibility UUID 37B30044-3B14-46BA-ABAA-F01000C27B63 for plug-in at path &lsquo;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/Unity4XC.xcplugin&rsquo; not present in DVTPlugInCompatibilityUUIDs</h3>

<p>A:There isn&rsquo;t official document about developing plugin for Xcode, so we can only attempt to solve it. Thanks to the internet, we can get a solution by other figure out.</p>

<pre><code>XCODEUUID=`defaults read /Applications/Xcode.app/Contents/Info DVTPlugInCompatibilityUUID`
for f in ~/Library/Application\ Support/Developer/Shared/Xcode/Plug-ins/*; do defaults write "$f/Contents/Info" DVTPlugInCompatibilityUUIDs -array-add $XCODEUUID; done
</code></pre>

<p>The reason probably is plugin developed compate with old Xcode, so it of course don&rsquo;t contain the lastest Xcode&rsquo;s UUID, we can manual add it if it really compate with the latest Xcode.</p>

<p>Reference: <a href="http://stackoverflow.com/questions/20732327/xcode-5-required-plug-in-not-present-in-dvtplugincompatibilityuuids?rq=1">Xcode 5 - Required plug-in not present in DVTPlugInCompatibilityUUIDs?</a></p>

<h3>7.Why AVCaptureSession output a wrong orientation?</h3>

<p>A:AVCaptureVideoPreviewLayer and AVCaptureOutput are different output destination, so we have to set oritentation for them each.</p>

<pre><code>let captureConnection = videoDataOutput.connection(withMediaType: AVMediaTypeVideo)

if captureConnection!.isVideoOrientationSupported {
captureConnection!.videoOrientation = AVCaptureVideoOrientation.portrait
}
else {
print("capture connection\(captureConnection!) doesn't support video orientation")
}
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/3561738/why-avcapturesession-output-a-wrong-orientation?rq=1">Why AVCaptureSession output a wrong orientation?</a><br/>
<a href="https://developer.apple.com/library/content/qa/qa1744/_index.html">Technical Q&amp;A QA1744 Setting the orientation of video with AV Foundation</a></p>

<h3>8.Operation stopped, the video could not be composed.</h3>

<pre><code>Domain=AVFoundationErrorDomain Code=-11841 "Operation Stopped" UserInfo=0x1912c320 {NSLocalizedDescription=Operation Stopped, NSLocalizedFailureReason=The video could not be composed.}
</code></pre>

<p>A: We should instance AVMutableComposition, AVMutableCompositionTrack every time when edit.</p>

<pre><code>mutableComposition = AVMutableComposition()

videoCompositionTrack = mutableComposition.addMutableTrack(withMediaType: AVMediaTypeVideo, preferredTrackID: kCMPersistentTrackID_Invalid)

audioCompositionTrack = mutableComposition.addMutableTrack(withMediaType: AVMediaTypeAudio, preferredTrackID: kCMPersistentTrackID_Invalid)
</code></pre>

<h3>9. How to get photos from iPhone simulator?</h3>

<p>A: Photo files are located at :</p>

<pre><code>~/Library/Developer/CoreSimulator/Devices/&lt;device UDID&gt;/data/Media/DCIM/100APPLE/
</code></pre>

<p>with Xcode 8.2. You can get <device UDID> from Devices window or using command:<code>xcrun simctl list devvices</code>.</p>

<p>Reference:<a href="http://stackoverflow.com/questions/5488915/getting-images-from-iphone-simulator">getting images from iPhone simulator</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[音频和视频格式]]></title>
    <link href="http://DamianSheldon.github.io/blog/audio-and-video-file-format.html"/>
    <updated>2016-10-21T15:15:53+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/audio-and-video-file-format</id>
    <content type="html"><![CDATA[<h2>音频格式</h2>

<p>我们通常说的音频格式准确地讲应该是音频文件格式，它是计算机系统上用于存放数字音频数据的文件格式，也可以看作一个容器。</p>

<p>音频数据的比特分布我们称为音频编码格式，它可以非压缩编码或压缩编码。压缩编码又分为无损压缩和有损压缩。</p>

<p>编码器(codec)就是来编解码原始音频数据的。</p>

<p>声音源 &ndash;ADC&ndash;> raw audio data &ndash;codec&ndash;> audio data(uncompressed/compressed) &ndash;packed&ndash;> audio file format(container format)</p>

<blockquote><p>An audio file format is a file format for storing digital audio data on a computer system. The bit layout of the audio data (excluding metadata) is called the audio coding format and can be uncompressed, or compressed to reduce the file size, often using lossy compression. The data can be a raw bitstream in an audio coding format, but it is usually embedded in a container format or an audio data format with defined storage layer.</p>

<p>It is important to distinguish between the audio coding format, the container containing the raw audio data, and an audio codec. A codec performs the encoding and decoding of the raw audio data while this encoded data is (usually) stored in a container file. Although most audio file formats support only one type of audio coding data (created with an audio coder), a multimedia container format (as Matroska or AVI) may support multiple types of audio and video data.</p>

<p>There are three major groups of audio file formats:
    • Uncompressed audio formats, such as WAV, AIFF, AU or raw header-less PCM;
    • Formats with lossless compression, such as FLAC, Monkey&rsquo;s Audio (filename extension .ape), WavPack (filename extension .wv), TTA, ATRAC Advanced Lossless, ALAC (filename extension .m4a), MPEG-4 SLS, MPEG-4 ALS, MPEG-4 DST, Windows Media Audio Lossless (WMA Lossless), and Shorten (SHN).
    • Formats with lossy compression, such as Opus, MP3, Vorbis, Musepack, AAC, ATRAC and Windows Media Audio Lossy (WMA lossy).</p></blockquote>

<!--more-->


<h2>视频格式</h2>

<p>视频文件格式是计算机系统上一种用来存放数字视频数据的文件格式。视频几乎都是以压缩格式的形式存储的以便减小文件大小。</p>

<p>视频文件格式也是一个容器，里面包含编码完的视频和音频数据，同样是使用编码器来完成编解码工作。</p>

<blockquote><p>A video file format is a type of file format for storing digital video data on a computer system. Video is almost always stored in compressed form to reduce the file size.</p>

<p>A video file normally consists of a container format (e.g. Matroska) containing video data in a video coding format (e.g. VP9) alongside audio data in an audio coding format (e.g. Opus). The container format can also contain synchronization information, subtitles, and metadata such as title. A standardized (or in some cases de facto standard) video file type such as .webm is a profilespecified by a restriction on which container format and which video and audio compression formats are allowed.</p>

<p>The coded video and audio inside a video file container (i.e. not headers, footers and metadata) is called the essence. A program (or hardware) which can decode video or audio is called a codec; playing or encoding a video file will sometimes require the user to install a codec library corresponding to the type of video and audio coding used in the file.</p></blockquote>

<h2>iOS and Android supported audio &amp; video codec formats</h2>

<h3>iOS</h3>

<p>iOS supports many industry-standard and Apple-specific audio formats, including the following:</p>

<ul>
<li>AAC</li>
<li>Apple Lossless (ALAC)</li>
<li>A-law</li>
<li>IMA/ADPCM (IMA4)</li>
<li>Linear PCM</li>
<li>µ-law</li>
<li>DVI/Intel IMA ADPCM</li>
<li>Microsoft GSM 6.10</li>
<li>AES3-2003</li>
</ul>


<p>Preferred Audio Formats in iOS</p>

<ul>
<li><p>For uncompressed (highest quality) audio, use 16-bit, little endian, linear PCM audio data packaged in a CAF file.</p></li>
<li><p>For compressed audio when playing one sound at a time, and when you don’t need to play audio simultaneously with the iPod application, use the AAC format packaged in a CAF or m4a file.</p></li>
<li>For less memory usage when you need to play multiple sounds simultaneously, use IMA4 (IMA/ADPCM) compression. This reduces file size but entails minimal CPU impact during decompression. As with linear PCM data, package IMA4 data in a CAF file.</li>
</ul>


<p>iOS supports many industry-standard video formats and compression standards, including the following:</p>

<ul>
<li>H.264 video, up to 1.5 Mbps, 640 by 480 pixels, 30 frames per second, Low-Complexity version of the H.264 Baseline Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>H.264 video, up to 768 Kbps, 320 by 240 pixels, 30 frames per second, Baseline Profile up to Level 1.3 with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>MPEG-4 video, up to 2.5 Mbps, 640 by 480 pixels, 30 frames per second, Simple Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>Numerous audio formats, including the ones listed in Audio Technologies</li>
</ul>


<h3>Android</h3>

<p>Audio</p>

<ul>
<li>AAC LC</li>
<li>HE-AACv1 (AAC+)</li>
<li>HE-AACv2 (enhanced AAC+)</li>
<li>AAC ELD (enhanced low delay AAC)</li>
<li>AMR-NB</li>
<li>AMR-WB</li>
<li>FLAC</li>
<li>MP3</li>
<li>MIDI</li>
<li>Vorbis</li>
<li>PCM/WAVE</li>
<li>Opus</li>
</ul>


<p>Video</p>

<ul>
<li>H.263</li>
<li>H.264 AVC</li>
<li>H.265 HEVC</li>
<li>MPEG-4 SP</li>
<li>VP8</li>
<li>VP9</li>
</ul>


<h3>iOS 和 Android 都支持的音频、视频格式</h3>

<p>Audio</p>

<ul>
<li>AAC LC (Low-Complexity profile)</li>
<li>Linear PCM</li>
</ul>


<p>Video</p>

<ul>
<li>H.264 AVC</li>
<li>MPEG-4 SP (Simple Profile)</li>
</ul>


<p>Profile</p>

<p>To address various applications ranging from low-quality, low-resolution surveillance cameras to high definition TV broadcasting and DVDs, many video standards group features into profiles and levels. MPEG-4 Part 2 has approximately 21 profiles, including profiles called Simple, Advanced Simple, Main, Core, Advanced Coding Efficiency, Advanced Real Time Simple, etc. The most commonly deployed profiles are Advanced Simple and Simple, which is a subset of Advanced Simple.</p>

<h2>Reference:</h2>

<p><a href="https://en.wikipedia.org/wiki/Audio_file_format">Audio file format</a><br/>
<a href="https://en.wikipedia.org/wiki/Video_file_format">Video file format</a><br/>
iOS Technology Overview<br/>
<a href="https://developer.android.com/guide/appendix/media-formats.html">Supported Media Formats</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS Technical Interview Part 4]]></title>
    <link href="http://DamianSheldon.github.io/blog/ios-technical-interview-part-4.html"/>
    <updated>2016-08-30T10:43:36+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/ios-technical-interview-part-4</id>
    <content type="html"><![CDATA[<h3>1.熟悉 CocoaPods 么？能大概讲一下工作原理么？</h3>

<p>A:熟悉，项目中的 Podfile 文件指定了使用的第三方库，根据它可以找到对应的 podspec 。 podspec 文件存储着第三方库的基本信息：包含哪些文件，静态库，资源文件，信赖哪些其他第三方库、系统框架，之后 pod 会创建一个工程，把这个库以 target 的形式包含进来，应用则依赖这个 pod 工程。</p>

<h3>2.最常用的版本控制工具是什么，能大概讲讲原理么？</h3>

<p>A: Git</p>

<h3>3.你一般是怎么用 Instruments 的？</h3>

<p>A:根据想要分析的问题，选择对应的　instruments　，同时参考　Instruments　User　Guide．</p>

<h3>4.你一般是如何调试 Bug 的</h3>

<p>A:复现 bug，之后单步调试定位。</p>

<h3>5.你在你的项目中用到了哪些设计模式？</h3>

<p>A:</p>

<pre><code>* 单例
* 委托
* Target-Action
* MVC
* 观察者
* 工厂方法
* 信息流
* 类簇
</code></pre>

<!-- more -->


<h3>6.如何实现单例，单例会有什么弊端？</h3>

<p>A：</p>

<pre><code>+ (instancetype)sharedInstance
{
    static dispatch_once_t once;
    static id sharedInstance;
    dispatch_once(&amp;once, ^{
        sharedInstance = [[self alloc] init];
    });
    return sharedInstance;
}
</code></pre>

<ul>
<li>单例相当于全局变量，因此存在强藕合，不利于扩展和应对变化；</li>
<li>单例违背单一设计原则；</li>
</ul>


<h3>7.如何把一个包含自定义对象的数组序列化到磁盘？</h3>

<p>A:自定义对象需要实现 NSCoding 协议，然后可以调用 NSKeyedArchiver 的<code>+ (BOOL)archiveRootObject:(id)rootObject toFile:(NSString *)path</code>方法将它序列化到磁盘。</p>

<h3>8.iOS 的沙盒目录结构是怎样的？ App Bundle 里面都有什么？</h3>

<p>A:</p>

<p>App Bundle 里面有 Info.plist, Executeable, Resource files and Other support files.</p>

<h3>9.iOS 7的多任务添加了哪两个新的 API? 各自的使用场景是什么？</h3>

<p>A:</p>

<ul>
<li>Apps that regularly update their content by contacting a server can register with the system and be launched periodically to retrieve that content in the background. To register, include the UIBackgroundModes key with the fetch value in your app’s Info.plist file. Then, when your app is launched, call the setMinimumBackgroundFetchInterval: method to determine how often it receives update messages. Finally, you must also implement the application:performFetchWithCompletionHandler: method in your app delegate.</li>
<li>Apps that use push notifications to notify the user that new content is available can fetch the content in the background. To support this mode, include the UIBackgroundModes key with the remote-notification value in your app’s Info.plist file. You must also implement the application:didReceiveRemoteNotification:fetchCompletionHandler: method in your app delegate.</li>
</ul>


<p>Reference: What&rsquo;s New in iOS</p>

<h3>10.Objective-C 的 class 是如何实现的？Selector 是如何被转化为 C 语言的函数调用的？</h3>

<p>A:</p>

<p>[receiver message]</p>

<p>=> objc_msgSend(receiver, selector);</p>

<h3>11.UIScrollView 大概是如何实现的，它是如何捕捉、响应手势的？</h3>

<h3>12.Objective-C 如何对已有的方法，添加自己的功能代码以实现类似记录日志这样的功能？</h3>

<p>A: Method Swizzle.</p>

<h3>13.<code>+load</code> 和 <code>+initialize</code> 的区别是什么？</h3>

<p>A:</p>

<ul>
<li>load:Invoked whenever a class or category is added to the Objective-C runtime; implement this method to perform class-specific behavior upon loading.</li>
<li>initialize:Initializes the class before it receives its first message.</li>
</ul>


<h3>14.NSOperation 相比于 GCD 有哪些优势？</h3>

<p>A:</p>

<ul>
<li>可以取消操作；</li>
<li>可以添加依赖；</li>
<li>可以 KVO 属性;</li>
<li>可以添加优先级;</li>
<li>可以复用;</li>
</ul>


<h3>15.strong / weak / unsafe_unretained 的区别</h3>

<p>A:</p>

<ul>
<li>strong:对象的引用计数加一;</li>
<li>weak:对象的引用计数不变，对象销毁时 weak 属性自动置为nil;</li>
<li>unsafe_unretained:对象的引用计数不变，对象销毁时 unsafe_unretained 属性成为野指针;</li>
</ul>


<p>你可能会奇怪，有了 weak 还要 unsafe_unretained 干什么？ 原因是 weak 是 iOS 5 才可用的，所以当你要兼容 iOS 5, 或者将 iOS 5 时代之前 MRC 代码迁移到 ARC 时会用它，除些之外我们应该使用 weak.</p>

<p>Reference:<a href="http://stackoverflow.com/questions/15968198/what-is-the-use-of-unsafe-unretained-attribute">http://stackoverflow.com/questions/15968198/what-is-the-use-of-unsafe-unretained-attribute</a></p>

<h3>16.Objective-C 中，meta-class 指的是什么</h3>

<p>A:类对象的类称为 meta-class.</p>

<h3>17.UIView 和 CALayer 之间的关系？</h3>

<p>A: UIView 会持有至少一个 CALayer 实例，CALayer 是 UIView 的骨架，它将 UIView 的内容绘制出来并提供动画支持。</p>

<h3>18.<code>+[UIView animateWithDuration:animations:completion:]</code> 内部大概是如何实现的？</h3>

<p>A:我觉得<code>+[UIView animateWithDuration:animations:completion:]</code> 内部应该是先使能 CALayer 实例的隐式动画，之后对 UIView animatable properties 的设置会传递到 CALayer 对应属性的 setter 方法，CALayer 的 setter 方法会触发对应的隐式动画，最后禁止 CALayer 实例的隐式动画。</p>

<p>Reference:</p>

<ul>
<li><a href="https://www.quora.com/How-is-UIViews-+animateWithDuration-animations-implemented">https://www.quora.com/How-is-UIViews-+animateWithDuration-animations-implemented</a></li>
<li><a href="http://stackoverflow.com/questions/15175750/how-uiview-animations-with-blocks-work-under-the-hood">http://stackoverflow.com/questions/15175750/how-uiview-animations-with-blocks-work-under-the-hood</a></li>
</ul>


<h3>19.什么时候会发生「隐式动画」？</h3>

<p>A: CALayer 的实例是支持隐式动画的，所以修改 CALayer Animatable Properties 里面的属性都可以触发隐式动画。UIView 默认禁止了 CALayer 的隐式动画，在动画块中又使能了，所以和 UIView 关联的 CALayer 实例只能在动画块中修改 Animatable UIView properties 里的属性也可以触发隐式动画。</p>

<p>Reference:<a href="http://stackoverflow.com/questions/4749343/when-exactly-do-implicit-animations-take-place-in-ios">http://stackoverflow.com/questions/4749343/when-exactly-do-implicit-animations-take-place-in-ios</a></p>

<h3>20.如何把一张大图缩小为1/4大小的缩略图？</h3>

<p>A:</p>

<ul>
<li>UIGraphicsBeginImageContextWithOptions &amp; UIImage -drawInRect:(UIKit)</li>
<li>CGBitmapContextCreate &amp; CGContextDrawImage(Core Graphics)</li>
<li>CGImageSourceCreateThumbnailAtIndex(Image IO)</li>
<li>Lanczos Resampling with Core Image(Core Image)</li>
<li>vImage in Accelerate(vImage)</li>
</ul>


<p>Reference:<a href="http://nshipster.com/image-resizing/">http://nshipster.com/image-resizing/</a></p>

<h3>21.Toll-Free Bridging 是什么？什么情况下会使用？</h3>

<p>A:</p>

<blockquote><p>There are a number of data types in the Core Foundation framework and the Foundation framework that can be used interchangeably. Data types that can be used interchangeably are also referred to as toll-free bridged data types. This means that you can use the same data structure as the argument to a Core Foundation function call or as the receiver of an Objective-C message invocation.</p></blockquote>

<h3>22.当系统出现内存警告时会发生什么？</h3>

<p>A:系统会杀死后台内存占用量大的应用释放内存给当前运行的应用，当前的应用也会收到内存警告的通知。</p>

<h3>23.什么是 Protocol，Delegate 一般是怎么用的？</h3>

<p>A:Protocol是一份消息合约。Delegate 是我们改变一个对象行为方式，这种方式不需要对类做继承。</p>

<h3>24.UIWebView 有哪些性能问题？有没有可替代的方案。</h3>

<p>A:</p>

<ul>
<li><strong>线程阻塞问题</strong>&ndash;当在 native 层调用 stringByEvaluatingJavaScriptFromString 方法时，可能由于 javascript 是单线程的原因，会阻塞原有 js 代码的执行。这里我们的解决办法是在 js 端用 defer 将 iframe 的插入延后执行。</li>
<li><strong>主线程的问题</strong>&ndash;UIWebView 的 stringByEvaluatingJavaScriptFromString 方法必须是主线程中执行，而主线程的执行时间过长就会 block UI 的更新。所以我们应该尽量让 stringByEvaluatingJavaScriptFromString 方法执行的时间短。</li>
</ul>


<p>Reference:<a href="http://blog.devtang.com/2012/03/24/talk-about-uiwebview-and-phonegap/">关于UIWebView的总结</a></p>

<h3>25.为什么 NotificationCenter 要 removeObserver? 如何实现自动 remove?</h3>

<p>A:因为之后产生了 Observer 感兴趣的通知 NotificationCenter 会调用 Observer 对应的处理方法，如果 Observer 销毁之后不从 NotificationCenter remove,那么会导致应用崩溃。</p>

<p>想要实现自动 remove，如果 NotificationCenter 对 Observer 的引用在 Observer 销毁后能自动置为 nil，类似 weak 的效果，那么问题就解决了。我们不能去修改 NotificationCenter 的内部实现，所以只能用其他的办法。有一个想法是在 Observer 销毁时调用 remove，可以利用 Objective-C 的 runtime 来帮助我们实现。具体思路是混写 addObserver 的方法，在这个混写方法中创建一个对象和 Observer 的生命周期关联起来，然后在这个关联对象的销毁方法中调用 removeObserver。</p>

<p>Reference:<a href="http://merowing.info/2012/03/automatic-removal-of-nsnotificationcenter-or-kvo-observers/">Automatic removal of NSNotificationCenter or KVO observers</a></p>

<h3>26.当 TableView 的 Cell 改变时，如何让这些改变以动画的形式呈现？</h3>

<p>A:You can also use beginUpdates method followed by the endUpdates method to animate the change in the row heights without reloading the cell.</p>

<h3>27.什么是 Method Swizzle，什么情况下会使用？</h3>

<p>A:Method Swizzle 是一种改变已存在方法实现的技术。当我们有改变已存在方法实现的需求是使用。</p>

<h3>28.为什么当 Core Animation 完成时，layer 又会恢复到原先的状态？</h3>

<p>A: Layer 有两个独立的图层: 1.modelLayer;2.presentationLayer. 动画是在 presentationLayer 上，所以当动画完成时， layer 又恢复到原先的状态。</p>

<h3>29.你会如何存储用户的一些敏感信息，如登录的 token。</h3>

<p>A:KeyChain services</p>

<h3>30.什么时候会使用 Core Graphics，有什么注意事项么？</h3>

<p>A:当需要绘制的内容不能用 UIKit 实现时，会使用 Core Graphics。</p>

<p>注意事项：</p>

<pre><code>* 坐标系统
* 线条的宽度尽量使用整数
* 缓存代价昂贵的数据
</code></pre>

<h3>31.使用 Block 时需要注意哪些问题？</h3>

<p>A:</p>

<ul>
<li>循环引用</li>
<li>对象的生命周期会延长</li>
</ul>


<h3>32.performSelector:withObject:afterDelay: 内部大概是怎么实现的，有什么注意事项么？</h3>

<p>A:</p>

<pre><code>- (void)eoc_performSelector:(SEL)aSelector
withObject:(id)anArgument
afterDelay:(NSTimeInterval)delay
{
    dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(delay * NSEC_PER_SEC)), dispatch_get_global_queue(QOS_CLASS_UTILITY, 0), ^{
            typedef void (*send_type)(id, SEL, id);

            send_type func = (send_type)objc_msgSend;
            func(self, aSelector, anArgument);
            });
}
</code></pre>

<p>注意事项：
可能会导致内存泄露。因为不确定调用的方法，所以编译器不能插入合适的内存管理方法调用。例如newObject，copy之类的方法调用就会导致内存泄露。</p>

<h3>33.如何播放 GIF 图片，有什么优化方案么？</h3>

<p>A: 用 ImageIO 框架为每一帧动画创建一个 UIImage 对象，把这些对象聚合成一个 animatedImages 赋给 UIImageView，然后根据 GIF 的文件信息设置好 animationDuration 和 animationRepeatCount，最后调用 startAnimating。</p>

<p>优化方案：</p>

<ul>
<li>variable frame delays &ndash; One approach to support variable frame delays with UIImageView is to find the greatest common divisor of all frame delays and slot longer frames multiple times in a row into the animatedImages array.</li>
<li>memory implications &ndash; Whenever memory is the constraint, instead of storing the solution to a problem one has to recalculate it. In our case we needed a way to load and decode the frames just in time before they were displayed, and to purge the ones that were no longer on screen.</li>
</ul>


<p>Reference:</p>

<ul>
<li><a href="http://engineering.flipboard.com/2014/05/animated-gif/">How FlipBoard Play Animated GIFs On iOS</a></li>
</ul>


<h3>34.有哪几种方式可以对图片进行缩放，使用 CoreGraphics 缩放时有什么注意事项？</h3>

<p>A:</p>

<ul>
<li>UIGraphicsBeginImageContextWithOptions &amp; UIImage -drawInRect:(UIKit)</li>
<li>CGBitmapContextCreate &amp; CGContextDrawImage(Core Graphics)</li>
<li>CGImageSourceCreateThumbnailAtIndex(Image IO)</li>
<li>Lanczos Resampling with Core Image(Core Image)</li>
<li>vImage in Accelerate(vImage)</li>
</ul>


<h3>35.哪些途径可以让 ViewController 瘦下来？</h3>

<p>A: 我们的 App 基本都是使用类 MVC 架构，要想让 ViewController 瘦下来，基本的想法肯定是把和 ViewController 弱相关的代码移出去，移到哪？要么是称多 MV 中， 要么移到一个辅助对象中。所以基本做法应该是把弱业务代码移到 Model 中，和 View 相关的代码移到 View 中；甚至引入一个辅助对象来分担可以分离出去的职责。</p>

<p>Reference: <a href="https://objccn.io/issue-1-1/">更轻量的 View Controllers</a></p>

<h3>36.有哪些常见的 Crash 场景？</h3>

<p>A:</p>

<ul>
<li>数组越界</li>
<li>向数组或字典中插入的对象为 nil</li>
<li>向已经销毁的对象发送消息</li>
<li>内存管理出错</li>
<li>调用一个对象不支持的方法</li>
<li>看门狗超时</li>
<li>在非主线程操作 UI</li>
</ul>


<h3>37.设计一套大文件（如上百M的视频）下载方案。</h3>

<p>A:
<a href="https://github.com/thibaultcha/TCBlobDownload">TCBlobDownload</a></p>

<h3>38.如果让你来实现 dispatch_once，你会怎么做？</h3>

<p>A:</p>

<pre><code>void SpinlockOnce(dispatch_once_t *predicate, dispatch_block_t block) {
        static OSSpinLock lock = OS_SPINLOCK_INIT;

        OSSpinLockLock(&amp;lock);
        if(!*predicate) {
            block();
            *predicate = 1;
        }
        OSSpinLockUnlock(&amp;lock);
    }
</code></pre>

<p>Reference:
<a href="https://www.mikeash.com/pyblog/friday-qa-2014-06-06-secrets-of-dispatch_once.html">Friday Q&amp;A 2014-06-06: Secrets of dispatch_once</a></p>

<p>Reference:<a href="https://github.com/lzyy/iOS-Developer-Interview-Questions">https://github.com/lzyy/iOS-Developer-Interview-Questions</a></p>
]]></content>
  </entry>
  
</feed>
