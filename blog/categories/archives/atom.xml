<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: archives | Hello World]]></title>
  <link href="http://DamianSheldon.github.io/blog/categories/archives/atom.xml" rel="self"/>
  <link href="http://DamianSheldon.github.io/"/>
  <updated>2017-04-05T10:06:05+08:00</updated>
  <id>http://DamianSheldon.github.io/</id>
  <author>
    <name><![CDATA[Sheldon]]></name>
    <email><![CDATA[dongmeilianghy@sina.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Android开发问题汇总(三)]]></title>
    <link href="http://DamianSheldon.github.io/blog/problems-of-android-development-part-3.html"/>
    <updated>2017-04-05T08:48:36+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/problems-of-android-development-part-3</id>
    <content type="html"><![CDATA[<h3>1. How to define custom attributes?</h3>

<p>A:Currently the best documentation is the source. You can take a look at it <a href="https://github.com/android/platform_frameworks_base/blob/master/core/res/res/values/attrs.xml">here(arrts.xml)</a>.</p>

<p>You can define attributes in the top <code>&lt;resources&gt;</code> element or inside of a <code>&lt;declare-styleable&gt;</code> element. If I&rsquo;m going to use an attr in more than on place I put it in the root element. Note , all attributes share the same global namespace. That means that even if you create a new attribute inside of a <code>&lt;declare-styleable&gt;</code> element it can be used outside of it and you cannot create another attribute with the same name of a different type.</p>

<p>An <code>&lt;attr&gt;</code> element has two xml attributes <code>name</code> and <code>format</code>. <code>name</code> lets you call it something and this how you end up refering to it in code, e.g., R.attr.my_attribute. The <code>format</code> attribute can have different values depending on the type of attribute you want.</p>

<ul>
<li>reference - if it references another resource id(e.g, &ldquo;@color/my_color&rdquo;, &ldquo;@layout/my_layout&rdquo;)</li>
<li>color</li>
<li>boolean</li>
<li>dimension</li>
<li>float</li>
<li>integer</li>
<li>string</li>
<li>fraction</li>
<li>enum - normally implicitly defined</li>
<li>flag - normally implicitly defined</li>
</ul>


<p>You can set the format to multiple types by using |, e.g., <code>format="reference|color"</code>.</p>

<p>enum attributes can be defined as follows:</p>

<pre><code>&lt;attr name="my_enum_attr"&gt; 
    &lt;enmu name="value1" value="1" /&gt;
    &lt;enmu name="value2" value="2" /&gt;
&lt;/attr&gt;
</code></pre>

<p>flag attributes are similar except the values need to defined so they can be bit ored together:</p>

<pre><code>&lt;attr name="my_flag_attr"&gt;
    &lt;flag name="fuzzy" value="0x01" /&gt;
    &lt;flag name="cold" value="0x02" /&gt;
&lt;/attr&gt;
</code></pre>

<p>In addition to attributes there is the <code>&lt;declare-styleable&gt;</code> element. This allows you to define attributes a custom view can use. You do this by specifying an <code>&lt;attr&gt;</code> element, if it was previously defined you do not specify the format. If you wish to reuse an android attr, for example android:gravity, then you can do that in the name, as follows.</p>

<p>An example of a custom view <code>&lt;declare-styleable&gt;</code>:</p>

<pre><code>&lt;declare-styleable name="MyCustomView"&gt; 
    &lt;attr name="my_custom_attribute" /&gt;
    &lt;attr name="android:gravity" /&gt;
&lt;/declare-styleable&gt;
</code></pre>

<p>When defining you custom attributes in XML on you need to do a few things.</p>

<p>First you must declare a namespace to find your attributes. You do this on the root layout element. Normal there is only <code>xmlns:android="http//schemas.android.com/apk/res/android"</code>. You must now also add <code>xmlns:app="http://schemas.android.com/apk/res-auto"</code>.</p>

<p>Example:</p>

<pre><code>&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;LinearLayout
xmlns:android="http://schemas.android.com/apk/res/android"
xmlns:whatever="http://schemas.android.com/apk/res-auto"
android:orientation="vertical"
android:layout_width="fill_parent"
android:layout_height="fill_parent"&gt;

&lt;org.example.mypackage.MyCustomView
android:layout_width="fill_parent"
android:layout_height="wrap_content"
android:gravity="center"
whatever:my_custom_attribute="Hello, world!" /&gt;
&lt;/LinearLayout&gt;
</code></pre>

<p>Finally, to access that custom attribute you normally do so in the constructor of you custom view as follows:</p>

<pre><code>public MyCustomView(Context context, AttributeSet attrs, int defStyle) {
    super(context, attrs, defStyle);

    TypedArray a = context.obtainStyledAttributes(attrs, R.styleable.MyCustomView, defStyle, 0);

    String str = a.getString(R.styleable.MyCustomView_my_custom_attribute);

    //do something with str

    a.recycle();
}
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/3441396/defining-custom-attrs">Defining custom attrs</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android 真机抓包]]></title>
    <link href="http://DamianSheldon.github.io/blog/capturing-android-physic-device-traffic-on-tcpdump.html"/>
    <updated>2017-03-11T17:04:33+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/capturing-android-physic-device-traffic-on-tcpdump</id>
    <content type="html"><![CDATA[<p>在 Android 开发过程中，可能会遇到和服务端交互有问题的情况，这时候就得拿出证据来和服务端撕逼, 而最有力的证据自然是抓取的网络数据包；又或者是遇到很诡异的网络问题，这时候就可以借助抓包来分析和定位问题。</p>

<p>如果我们和服务端的交互没有通过 VPN, 而且也不是视频流这种网络性能要求苛刻的情况，我们可以通过 tPacketCapture 这种应用来抓包；</p>

<p>其他情况我们可以通过 root 手机，然后安装 tcpdump 来抓包。</p>

<p>下面我们详细介绍下 通过 tcpdump 抓包这种方法：</p>

<ul>
<li>Root 手机</li>
</ul>


<p>Root 手机的原理是利用系统存在的漏洞来获得 root 权限，<a href="https://forum.xda-developers.com/">XDA Developers</a> 上有不少 root 工具，很多手机都可以用它们 root。</p>

<ul>
<li>安装 tcpdump</li>
</ul>


<p>可以到网上搜索为 Android 编译好的 tcpdump 二进制包，例如<a href="http://www.strazzere.com/android/tcpdump">这里</a>就有一个。</p>

<pre><code>// Copy tcpdump to device
$ adb -d push /path/to/tcpdump /sdcard/tcpdump

// Device shell
$ adb -d shell

// Switch to root
$ su

// Copy tcpdump to /data/local/
# cat /sdcard/tcpdump /data/local/tcpdump
</code></pre>

<!--more-->


<ul>
<li>抓包</li>
</ul>


<pre><code>/# cd /data/local
/# ./tcpdump -i any -p -s 0 -w /sdcard/capture.pcap

//  Options
    # "-i any": listen on any network interface

　　# "-p": disable promiscuous mode (doesn't work anyway)

　　# "-s 0": capture the entire packet

　　# "-w": write packets to a file (rather than printing to stdout)

　　... do whatever you want to capture, then ^C to stop it ...
</code></pre>

<ul>
<li>分析</li>
</ul>


<pre><code>// Copy capture.pcap to computer
$ adb -d pull /sdcard/capture.pcap /path/to/capture.pcap

Analyze with Wireshark.
</code></pre>

<ul>
<li>Shell Commands</li>
</ul>


<p>Android 手机上的命令通常不全，我们可以通过安装 BusyBox 来提供一个相对完成 Shell 命令集方便我们的开发工作。</p>

<p>1,Download <a href="http://www.busybox.net/downloads/binaries">BusyBox</a> 的压缩包;<br/>
2,获取设备 CPU 的架构版本 <code>adb -d shell cat /proc/cpuinfo</code><br/>
3,解开压缩包，把对应 CPU 架构版本的二进制包生命名为 busybox,例如 <code>mv busybox-armv7l busybox</code>;<br/>
4,安装 busybox 到设备上，</p>

<pre><code>// Copy busybox to device
$adb -d push /path/to/busybox /sdcard/busybox

// Switch to device shell
$adb -d shell

// Install busybox
$ su

\# cat /sdcard/busybox /system/xbin/busybox

// Check install result
# busybox 

...
</code></pre>

<h2>Reference</h2>

<ul>
<li><a href="http://www.cnblogs.com/likwo/archive/2012/09/06/2673944.html">Android通过tcpdump抓包</a></li>
<li><a href="http://www.cnblogs.com/blues_/p/3582097.html">转adb Shell root 权限</a></li>
<li><a href="http://www.cnblogs.com/xiaowenji/archive/2011/03/12/1982309.html">为Android安装BusyBox —— 完整的bash shell</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Photos 框架的基本使用]]></title>
    <link href="http://DamianSheldon.github.io/blog/photos-framework-usage.html"/>
    <updated>2016-12-23T14:23:33+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/photos-framework-usage</id>
    <content type="html"><![CDATA[<p>从 iOS 9 开始 Apple 把 Asset Library 标记为废弃状态，并建议开发者使用 Photos 框架。</p>

<blockquote><p>The Assets Library framework is deprecated as of iOS 9.0. Instead, use the Photos framework instead, which in iOS 8.0 and later provides more features and better performance for working with a user’s photo library.</p></blockquote>

<p>不幸的是 Apple 并没有发布相关的使用指导文档，只有一个相关 Demo。使用的时候固然可以回头参考这个 Demo，但这样的效率不是很高，很多概念也容易忘记，所以这里做个简单的总结。</p>

<p>Photos 中有不少类，其中几个犹为关键。PHPhotoLibary 是我们操作 Photo Library 里面资源的入口对象，所有的操作都通过它完成。PHCollectionList 表示相册中的专题列表；PHAssetCollection 表示专题；PHAsset 表示资源，如 images, videos, and Live Photos.</p>

<p>我们基本的需求就是 CRUD, 这些操作是需要用户授权的，记得先获取权限再操作， 下面我们展示相关的代码片段。</p>

<h3>Create</h3>

<ol>
<li>创建一个资源</li>
</ol>


<pre><code>PHPhotoLibrary.shared().performChanges({
            PHAssetChangeRequest.creationRequestForAsset(from: image)
        }, completionHandler: {success, error in
            if !success { print("error creating asset: \(error)") }
        })
</code></pre>

<ol>
<li>创建一个资源到指定的专题</li>
</ol>


<pre><code>PHPhotoLibrary.shared().performChanges({
            let creationRequest = PHAssetChangeRequest.creationRequestForAsset(from: image)
            if let assetCollection = self.assetCollection {
            let addAssetRequest = PHAssetCollectionChangeRequest(for: assetCollection)
            addAssetRequest?.addAssets([creationRequest.placeholderForCreatedAsset!] as NSArray)
            }
        }, completionHandler: {success, error in
            if !success { print("error creating asset: \(error)") }
        })
</code></pre>

<!-- more -->


<h3>Read (Fetch)</h3>

<p>获取资源是通过 PHAsset 提供的一系列以 fetchXXX 开头的类方法，选择哪个方法取决于需求，这里示例其中两个我觉得常用的方法。</p>

<ol>
<li><code>class func fetchAssets(with options: PHFetchOptions?) -&gt; PHFetchResult&lt;PHAsset&gt;</code></li>
</ol>


<p>我们可以用这个方法获取 Photo Library 里面所有的资源。</p>

<pre><code>let allPhotosOptions = PHFetchOptions()
    allPhotosOptions.sortDescriptors = [NSSortDescriptor(key: "creationDate", ascending: true)]
self.fetchResult = PHAsset.fetchAssets(with: allPhotosOptions)
</code></pre>

<ol>
<li><code>class func fetchAssets(in assetCollection: PHAssetCollection, options: PHFetchOptions?) -&gt; PHFetchResult&lt;PHAsset&gt;</code></li>
</ol>


<p>我们可以用这个方法获取指定专题里面的资源。例如我们想获取 Camera Roll 这个专题里面的资源：</p>

<pre><code>let cameraRoll: PHFetchResult&lt;PHAssetCollection&gt; = PHAssetCollection.fetchAssetCollections(with: .smartAlbum, subtype: .smartAlbumUserLibrary, options: nil).firstObject
let fetchResult = PHAsset.fetchAssets(in: cameraRoll, options: nil)
</code></pre>

<h3>Update (Edit)</h3>

<p>编辑的基本的做法是先用资源请求一个 PHContentEditingInput，然后编辑资源，为了方便用户之后继续编辑或撤销可以实例化一个 PHAdjustmentData 对象来持有相关信息。编辑完成之后对于图片和视频需要实例化一个 PHContentEditingOutput 来完成输出，PHContentEditingOutput 的 adjustmentData 属性关联之前的 PHAdjustmentData, 并把编辑完成的内容输出到 PHContentEditingOutput 的 renderedContentURL。最后创建一个 PHAssetChangeRequest 对象，设置它的 contentEditingOutput 为
之前实例化的 PHContentEditingOutput。</p>

<p>这部分的代码会多点，具体可以查看 <a href="https://github.com/DamianSheldon/PhotosFrameworkUsage">Demo</a>.</p>

<h3>Delete</h3>

<pre><code>PHPhotoLibrary.shared().performChanges({ 
        PHAssetChangeRequest.deleteAssets([self.asset] as NSArray)
        }) { (success, error) in
    DispatchQueue.main.sync {
        self.trashButton.isEnabled = success ? false : true
    }

    if success {
        print("delete asset successfully")
    }
    else {
        print("can't delete asset: \(error)")
    }
}
</code></pre>

<h3>完整 Demo</h3>

<p><a href="https://github.com/DamianSheldon/PhotosFrameworkUsage">PhotosFrameworkUsage</a></p>

<h3>Caveat</h3>

<p>使用过程中遇到一个坑，这里记一下。</p>

<pre><code>guard let inputImage = CIImage(contentsOf: input.fullSizeImageURL!)
            else { fatalError("can't load input image to edit") }

// Apply the filter.
let outputImage = inputImage
    .applyingOrientation(input.fullSizeImageOrientation)
.applyingFilter(filterName, withInputParameters: nil)

// List 1.
let uiImage = UIImage(ciImage: outputImage)

// List 2.
if let cgImage = CIContext(options: nil).createCGImage(outputImage, from: outputImage.extent) {
    let uiImage = UIImage(cgImage:cgImage)
}
else {
    print("instance UIImage from CGImage failed!")    
}

// Ouput
if let data = UIImageJPEGRepresentation(uiImage, 0.7) {
    // NSData - (BOOL)writeToURL:(NSURL *)url atomically:(BOOL)atomically;
    do {
        try data.write(to: output.renderedContentURL)

    } catch let error {
        print("output filtered image to specify URL failed: \(error)")
    }
}
else {
    print("generate JPEG representation data failed")
        return
}
</code></pre>

<p>这里的问题是直接用 CIImage 实例化  UIImage 会失败，得转成 CGImage 然后实例化 UIImage. 至于它的原因我暂时还不清楚。</p>

<p>Reference:<a href="http://stackoverflow.com/questions/29732886/uiimagejpegrepresentation-returns-nil">UIImageJPEGRepresentation returns nil</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS 开发问题汇总(九)]]></title>
    <link href="http://DamianSheldon.github.io/blog/ios-development-problems-part-9.html"/>
    <updated>2016-11-15T14:49:17+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/ios-development-problems-part-9</id>
    <content type="html"><![CDATA[<h3>1.Curried functions in Swift</h3>

<p>A:There’s a difference between self.methodname (which you are using), and Classname.methodname.</p>

<p>The former, when called within a class’s method, will give you a function bound with that class instance. So if you call it, it will be called on that instance.</p>

<p>The latter gives you a curried function that takes as an argument any instance of Classname, and returns a new function that is bound to that instance. At this point, that function is like the first case (only you can bind it to any instance you like).</p>

<p>Here’s an example to try and show that a bit better:</p>

<pre><code>class C {
    private let _msg: String
        init(msg: String) { _msg = msg }

    func c_print() { print(_msg) }

    func getPrinter() -&gt; ()-&gt;() { return self.c_print }
}

let c = C(msg: "woo-hoo")
let f = c.getPrinter()
// f is of type (())-&gt;()
f() // prints "woo-hoo"

let d = C(msg: "way-hey")

let g = C.c_print
// g is of type (C) -&gt; (()) -&gt; (),
// you need to feed it a C:
g(c)() // prints "woo-hoo"
g(d)() // prints "way-hey"

// instead of calling immediately,
// you could store the return of g:
let h = g(c)
// at this point, f and h amount to the same thing:
// h is of type (())-&gt;()
h() // prints "woo-hoo"
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/27644702/curried-functions-in-swift">Curried functions in SWIFT</a></p>

<h3>2.NSLog on devices in iOS 10 / Xcode 8 will truncate.</h3>

<p>A:A temporary solution, just redefine all NSLOG to printf in a global header file.</p>

<pre><code>#define NSLog(FORMAT, ...) printf("%s\n", [[NSString stringWithFormat:FORMAT, ##__VA_ARGS__] UTF8String]);
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/39584707/nslog-on-devices-in-ios-10-xcode-8-seems-to-truncate-why">NSLog on devices in iOS 10 / Xcode 8 seems to truncate? Why？</a></p>

<!--more-->


<h3>3.UIImagePickerViewController is black doesn&rsquo;t work for selecting photo or taking picture.</h3>

<p>A:I instance it via <code>UIImagePickerViewController(nibName:nil, boundle:nil)</code> that doesn&rsquo;t work, and change to <code>UIImagePickerViewController()</code> it works.</p>

<h3>4.How to create dispatch queue in Swift?</h3>

<p>A:</p>

<pre><code>// Concurrent dispatch queue
let concurrentQueue = DispatchQueue(label: "queuename", attributes:.concurrent)

// Serial dispatch queue
let serialQueue = DispatchQueue(label: "queuename")
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/37805885/how-to-create-dispatch-queue-in-swift-3">How to create dispatch queue in Swift 3</a></p>

<h3>5.How to create a bitmap graphics context in Swift 3?</h3>

<p>A:</p>

<pre><code>// Create a bitmap graphics context with the sample buffer data
guard let context = CGContext(data: baseAddress, width: width, height: height, bitsPerComponent: 8,
        bytesPerRow: bytesPerRow, space: colorSpace, bitmapInfo: CGImageAlphaInfo.premultipliedFirst.rawValue | CGBitmapInfo.byteOrder32Little.rawValue) else {
    return nil
}
</code></pre>

<p>Reference: <a href="http://stackoverflow.com/questions/24109149/cgbitmapcontextcreate-error-with-swift">CGBitmapContextCreate error with swift</a></p>

<h3>6.Required plug-in compatibility UUID 37B30044-3B14-46BA-ABAA-F01000C27B63 for plug-in at path &lsquo;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/Unity4XC.xcplugin&rsquo; not present in DVTPlugInCompatibilityUUIDs</h3>

<p>A:There isn&rsquo;t official document about developing plugin for Xcode, so we can only attempt to solve it. Thanks to the internet, we can get a solution by other figure out.</p>

<pre><code>XCODEUUID=`defaults read /Applications/Xcode.app/Contents/Info DVTPlugInCompatibilityUUID`
for f in ~/Library/Application\ Support/Developer/Shared/Xcode/Plug-ins/*; do defaults write "$f/Contents/Info" DVTPlugInCompatibilityUUIDs -array-add $XCODEUUID; done
</code></pre>

<p>The reason probably is plugin developed compate with old Xcode, so it of course don&rsquo;t contain the lastest Xcode&rsquo;s UUID, we can manual add it if it really compate with the latest Xcode.</p>

<p>Reference: <a href="http://stackoverflow.com/questions/20732327/xcode-5-required-plug-in-not-present-in-dvtplugincompatibilityuuids?rq=1">Xcode 5 - Required plug-in not present in DVTPlugInCompatibilityUUIDs?</a></p>

<h3>7.Why AVCaptureSession output a wrong orientation?</h3>

<p>A:AVCaptureVideoPreviewLayer and AVCaptureOutput are different output destination, so we have to set oritentation for them each.</p>

<pre><code>let captureConnection = videoDataOutput.connection(withMediaType: AVMediaTypeVideo)

if captureConnection!.isVideoOrientationSupported {
captureConnection!.videoOrientation = AVCaptureVideoOrientation.portrait
}
else {
print("capture connection\(captureConnection!) doesn't support video orientation")
}
</code></pre>

<p>Reference:<a href="http://stackoverflow.com/questions/3561738/why-avcapturesession-output-a-wrong-orientation?rq=1">Why AVCaptureSession output a wrong orientation?</a><br/>
<a href="https://developer.apple.com/library/content/qa/qa1744/_index.html">Technical Q&amp;A QA1744 Setting the orientation of video with AV Foundation</a></p>

<h3>8.Operation stopped, the video could not be composed.</h3>

<pre><code>Domain=AVFoundationErrorDomain Code=-11841 "Operation Stopped" UserInfo=0x1912c320 {NSLocalizedDescription=Operation Stopped, NSLocalizedFailureReason=The video could not be composed.}
</code></pre>

<p>A: We should instance AVMutableComposition, AVMutableCompositionTrack every time when edit.</p>

<pre><code>mutableComposition = AVMutableComposition()

videoCompositionTrack = mutableComposition.addMutableTrack(withMediaType: AVMediaTypeVideo, preferredTrackID: kCMPersistentTrackID_Invalid)

audioCompositionTrack = mutableComposition.addMutableTrack(withMediaType: AVMediaTypeAudio, preferredTrackID: kCMPersistentTrackID_Invalid)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[音频和视频格式]]></title>
    <link href="http://DamianSheldon.github.io/blog/audio-and-video-file-format.html"/>
    <updated>2016-10-21T15:15:53+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/audio-and-video-file-format</id>
    <content type="html"><![CDATA[<h2>音频格式</h2>

<p>我们通常说的音频格式准确地讲应该是音频文件格式，它是计算机系统上用于存放数字音频数据的文件格式，也可以看作一个容器。</p>

<p>音频数据的比特分布我们称为音频编码格式，它可以非压缩编码或压缩编码。压缩编码又分为无损压缩和有损压缩。</p>

<p>编码器(codec)就是来编解码原始音频数据的。</p>

<p>声音源 &ndash;ADC&ndash;> raw audio data &ndash;codec&ndash;> audio data(uncompressed/compressed) &ndash;packed&ndash;> audio file format(container format)</p>

<blockquote><p>An audio file format is a file format for storing digital audio data on a computer system. The bit layout of the audio data (excluding metadata) is called the audio coding format and can be uncompressed, or compressed to reduce the file size, often using lossy compression. The data can be a raw bitstream in an audio coding format, but it is usually embedded in a container format or an audio data format with defined storage layer.</p>

<p>It is important to distinguish between the audio coding format, the container containing the raw audio data, and an audio codec. A codec performs the encoding and decoding of the raw audio data while this encoded data is (usually) stored in a container file. Although most audio file formats support only one type of audio coding data (created with an audio coder), a multimedia container format (as Matroska or AVI) may support multiple types of audio and video data.</p>

<p>There are three major groups of audio file formats:
    • Uncompressed audio formats, such as WAV, AIFF, AU or raw header-less PCM;
    • Formats with lossless compression, such as FLAC, Monkey&rsquo;s Audio (filename extension .ape), WavPack (filename extension .wv), TTA, ATRAC Advanced Lossless, ALAC (filename extension .m4a), MPEG-4 SLS, MPEG-4 ALS, MPEG-4 DST, Windows Media Audio Lossless (WMA Lossless), and Shorten (SHN).
    • Formats with lossy compression, such as Opus, MP3, Vorbis, Musepack, AAC, ATRAC and Windows Media Audio Lossy (WMA lossy).</p></blockquote>

<!--more-->


<h2>视频格式</h2>

<p>视频文件格式是计算机系统上一种用来存放数字视频数据的文件格式。视频几乎都是以压缩格式的形式存储的以便减小文件大小。</p>

<p>视频文件格式也是一个容器，里面包含编码完的视频和音频数据，同样是使用编码器来完成编解码工作。</p>

<blockquote><p>A video file format is a type of file format for storing digital video data on a computer system. Video is almost always stored in compressed form to reduce the file size.</p>

<p>A video file normally consists of a container format (e.g. Matroska) containing video data in a video coding format (e.g. VP9) alongside audio data in an audio coding format (e.g. Opus). The container format can also contain synchronization information, subtitles, and metadata such as title. A standardized (or in some cases de facto standard) video file type such as .webm is a profilespecified by a restriction on which container format and which video and audio compression formats are allowed.</p>

<p>The coded video and audio inside a video file container (i.e. not headers, footers and metadata) is called the essence. A program (or hardware) which can decode video or audio is called a codec; playing or encoding a video file will sometimes require the user to install a codec library corresponding to the type of video and audio coding used in the file.</p></blockquote>

<h2>iOS and Android supported audio &amp; video codec formats</h2>

<h3>iOS</h3>

<p>iOS supports many industry-standard and Apple-specific audio formats, including the following:</p>

<ul>
<li>AAC</li>
<li>Apple Lossless (ALAC)</li>
<li>A-law</li>
<li>IMA/ADPCM (IMA4)</li>
<li>Linear PCM</li>
<li>µ-law</li>
<li>DVI/Intel IMA ADPCM</li>
<li>Microsoft GSM 6.10</li>
<li>AES3-2003</li>
</ul>


<p>Preferred Audio Formats in iOS</p>

<ul>
<li><p>For uncompressed (highest quality) audio, use 16-bit, little endian, linear PCM audio data packaged in a CAF file.</p></li>
<li><p>For compressed audio when playing one sound at a time, and when you don’t need to play audio simultaneously with the iPod application, use the AAC format packaged in a CAF or m4a file.</p></li>
<li>For less memory usage when you need to play multiple sounds simultaneously, use IMA4 (IMA/ADPCM) compression. This reduces file size but entails minimal CPU impact during decompression. As with linear PCM data, package IMA4 data in a CAF file.</li>
</ul>


<p>iOS supports many industry-standard video formats and compression standards, including the following:</p>

<ul>
<li>H.264 video, up to 1.5 Mbps, 640 by 480 pixels, 30 frames per second, Low-Complexity version of the H.264 Baseline Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>H.264 video, up to 768 Kbps, 320 by 240 pixels, 30 frames per second, Baseline Profile up to Level 1.3 with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>MPEG-4 video, up to 2.5 Mbps, 640 by 480 pixels, 30 frames per second, Simple Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</li>
<li>Numerous audio formats, including the ones listed in Audio Technologies</li>
</ul>


<h3>Android</h3>

<p>Audio</p>

<ul>
<li>AAC LC</li>
<li>HE-AACv1 (AAC+)</li>
<li>HE-AACv2 (enhanced AAC+)</li>
<li>AAC ELD (enhanced low delay AAC)</li>
<li>AMR-NB</li>
<li>AMR-WB</li>
<li>FLAC</li>
<li>MP3</li>
<li>MIDI</li>
<li>Vorbis</li>
<li>PCM/WAVE</li>
<li>Opus</li>
</ul>


<p>Video</p>

<ul>
<li>H.263</li>
<li>H.264 AVC</li>
<li>H.265 HEVC</li>
<li>MPEG-4 SP</li>
<li>VP8</li>
<li>VP9</li>
</ul>


<h3>iOS 和 Android 都支持的音频、视频格式</h3>

<p>Audio</p>

<ul>
<li>AAC LC (Low-Complexity profile)</li>
<li>Linear PCM</li>
</ul>


<p>Video</p>

<ul>
<li>H.264 AVC</li>
<li>MPEG-4 SP (Simple Profile)</li>
</ul>


<p>Profile</p>

<p>To address various applications ranging from low-quality, low-resolution surveillance cameras to high definition TV broadcasting and DVDs, many video standards group features into profiles and levels. MPEG-4 Part 2 has approximately 21 profiles, including profiles called Simple, Advanced Simple, Main, Core, Advanced Coding Efficiency, Advanced Real Time Simple, etc. The most commonly deployed profiles are Advanced Simple and Simple, which is a subset of Advanced Simple.</p>

<h2>Reference:</h2>

<p><a href="https://en.wikipedia.org/wiki/Audio_file_format">Audio file format</a><br/>
<a href="https://en.wikipedia.org/wiki/Video_file_format">Video file format</a><br/>
iOS Technology Overview<br/>
<a href="https://developer.android.com/guide/appendix/media-formats.html">Supported Media Formats</a></p>
]]></content>
  </entry>
  
</feed>
