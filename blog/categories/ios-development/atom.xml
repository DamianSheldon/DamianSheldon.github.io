<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ios development | Hello World]]></title>
  <link href="http://DamianSheldon.github.io/blog/categories/ios-development/atom.xml" rel="self"/>
  <link href="http://DamianSheldon.github.io/"/>
  <updated>2017-11-27T10:02:04+08:00</updated>
  <id>http://DamianSheldon.github.io/</id>
  <author>
    <name><![CDATA[Sheldon]]></name>
    <email><![CDATA[dongmeilianghy@sina.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[圆锥渐变的一种简单实现]]></title>
    <link href="http://DamianSheldon.github.io/blog/a-simple-conical-gradient-on-ios.html"/>
    <updated>2017-09-22T15:59:35+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/a-simple-conical-gradient-on-ios</id>
    <content type="html"><![CDATA[<p>Core Graphics 支持两种渐变：线性(Axial)和径向(Radial)渐变，但是有的时候我们可能会用到圆锥(Conical)渐变，例如在扫描附近的目标时，交互可能用上带这种渐变的雷达效果，它长这样：
<img src="../images/Conical-1.png" alt="Conical-1" />
<img src="../images/Conical-2.png" alt="Conical-2" /></p>

<p>要实现这样一种渐变你会怎么做呢？我的想法是从渐变的本质着手。渐变是从一种颜色渐渐变化成另外一种颜色，而圆锥渐变是根据角度渐渐变化。我们把界面看成位图，这样可以由点的位置得到它的角度，继而根据角度线性插值可以得到它的颜色，最终就可以得到圆锥渐变。</p>

<p>想法有了，接下来我们用它来实现上图中 Find My iPhone 图标的雷达效果吧。</p>

<p>首先定义一个 CALayer 的子类 ConicalLayer，</p>

<!--more-->


<pre><code>// ConicalLayer.h
@interface ConicalLayer : CALayer

/// An array of CGColorRef objects defining the color of each gradient stop. 
@property(copy) NSArray *colors;

@end

// ConicalLayer.m
- (id)init
{
    if (!(self = [super init])) {
        return nil;
    }

    _needsDisplayOnBoundsChange = YES;

    return self;
}

- (void)drawInContext:(CGContextRef)ctx
{
    // Draw background
    CGRect rect = CGContextGetClipBoundingBox(ctx);
    CGContextSetFillColorWithColor(ctx, self.backgroundColor);
    CGContextFillRect(ctx, rect);

    if (self.colors.count &lt; 1) {
        return;
    }
    else if (self.colors.count &lt; 2) {
        // There is only one color so directly draw with it
        CGColorRef color = (__bridge CGColorRef)(self.colors.firstObject);
        CGContextSetFillColorWithColor(ctx, color);
        CGContextFillRect(ctx, rect);
        return;
    }

    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();

    size_t width = rect.size.width;
    size_t height = rect.size.height;

    size_t bitsPerCompoent = 8;
    size_t bytesPerRow = width * 4;

    size_t bitmapByteCount = bytesPerRow * height;

    uint32_t *bitmapData = calloc( bitmapByteCount / sizeof(uint32_t), sizeof(uint32_t) );

    // Map color to linear array each compoent occupy 1 byte
    uint8_t *colorCompoents = calloc(self.colors.count * 4, sizeof(uint8_t));

    for (int i = 0; i &lt; self.colors.count; ++i) {
        CGColorRef c = (__bridge CGColorRef)(self.colors[i]);

        const CGFloat *compoents = CGColorGetComponents(c);

        uint8_t red = compoents[0] * 255;
        uint8_t green = compoents[1] * 255;
        uint8_t blue = compoents[2] * 255;
        uint8_t alpha = compoents[3] * 255;

        int index = i * 4;
        *(colorCompoents + index) = red;
        *(colorCompoents + index + 1) = green;
        *(colorCompoents + index + 2) = blue;
        *(colorCompoents + index + 3) = alpha;
    }

    // Creating a Bitmap Graphics Context for conical gradient
    CGBitmapInfo bitmapInfo = kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Little;

    CGContextRef bitmapGraphicsCtx = CGBitmapContextCreate(bitmapData, width, height, bitsPerCompoent, bytesPerRow, colorSpace, bitmapInfo);

    // Creating conical gradient from a Bitmap Graphics Context
    CGImageRef conicalGradientImage = CGBitmapContextCreateImage(bitmapGraphicsCtx);

    CGContextRelease(bitmapGraphicsCtx);

    free(colorCompoents);

    free(bitmapData);

    CGColorSpaceRelease(colorSpace);

    // Draws conical gradient image into a graphics context.
    CGContextDrawImage(ctx, rect, conicalGradientImage);

    CGImageRelease(conicalGradientImage);

    // Draws three concentric
    CGContextBeginPath(ctx);

    CGFloat halfWidth = 0.5 * CGRectGetWidth(rect);
    CGFloat maxRadii = 0.8 * halfWidth;
    CGFloat radii = floor(0.33 * maxRadii);

    for (int i = 1; i &lt; 4; ++i) {
        CGFloat r = radii * i;
        CGFloat dx = halfWidth - r;
        CGRect ellipseRect = CGRectInset(rect, dx, dx);

        CGContextAddEllipseInRect(ctx, ellipseRect);
    }

    CGContextSetRGBStrokeColor(ctx, 41/255.0, 234/255.0, 35/255.0, 1.0);
    CGContextStrokePath(ctx);
}

- (BOOL)needsDisplayOnBoundsChange
{
    return _needsDisplayOnBoundsChange;
}

- (void)setNeedsDisplayOnBoundsChange:(BOOL)needsDisplayOnBoundsChange
{
}
</code></pre>

<p>现在我们把架子搭起来了，但是还没有往位图里面填充颜色，在这之前，对这段代码稍作解释，首先是我覆盖了needsDisplayOnBoundsChange 属性的 getter 和 setter 方法，原因是我设置图层关联的背景颜色时会触发这个属性变 NO，导致 <code>drawInContext</code> 不会被调用，我认为这是 Apple 的一个 bug，已经作了反馈，所以这里我使用了这么一个绕过的方法。</p>

<p>其次，我把 RGB 颜色空间的颜色分量取出来放在了一个一维数组里用来备用；最后是 bitmapInfo 要或上 kCGBitmapByteOrder32Little，不然结果会不正确。</p>

<p>接下来就是要填充位图的颜色，代码如下：</p>

<pre><code>    // Create conical gradient bitmap data
    CGFloat centerX = width * 0.5;
    CGFloat centerY = height * 0.5;

    double baseAngle = 2*M_PI / (self.colors.count - 1);

    for (int i = 0; i &lt; height; ++i) {
        for (int j = 0; j &lt; width; ++j) {
            CGFloat x = j - centerX;
            CGFloat y = i - centerY;

            // define atan2 uniquely one uses the principal value in the range (−π, π]. That is, −π &lt; atan2(y, x) ≤ π.
            double angle = atan2(y, x);

            // Convert atan2 result angle to range [0, 2π]
            if (angle &lt; 0) {
                angle += 2 * M_PI;
            }

            // 0-360 map to linear gradient
            double angleRatio = angle / baseAngle;
            int colorIndex = angleRatio; // How many times of base angle?

            angle -= colorIndex * baseAngle;
            angleRatio = angle / baseAngle;

            colorIndex *= 4;

            uint8_t red0 = colorCompoents[colorIndex];
            uint8_t red1 = colorCompoents[colorIndex + 4];

            // Green index
            colorIndex += 1;
            uint8_t green0 = colorCompoents[colorIndex];
            uint8_t green1 = colorCompoents[colorIndex + 4];

            // Blue index
            colorIndex += 1;
            uint8_t blue0 = colorCompoents[colorIndex];
            uint8_t blue1 = colorCompoents[colorIndex + 4];

            // Alpha index
            colorIndex += 1;
            uint8_t alpha0 = colorCompoents[colorIndex];
            uint8_t alpha1 = colorCompoents[colorIndex + 4];

//            uint8_t red = red0 + angleRatio * (red1 - red0);
//            uint8_t green = green0 + angleRatio * (green1 - green0);
//            uint8_t blue = blue0 + angleRatio * (blue1 - blue0);
//            uint8_t alpha = alpha0 + angleRatio * (alpha1 - alpha0);

            uint8_t red = lerp(red0, red1, angleRatio);
            uint8_t green = lerp(green0, green1, angleRatio);
            uint8_t blue = lerp(blue0, blue1, angleRatio);
            uint8_t alpha = lerp(alpha0, alpha1, angleRatio);

            // Multiple alpha
            float a = alpha / 255.0;

            red *= a;
            green *= a;
            blue *= a;

            unsigned long index = i * width + j;

            *(bitmapData + index) = (red &lt;&lt; 24) | (green &lt;&lt; 16) | (blue &lt;&lt; 8) | alpha;
        }
    }
</code></pre>

<p>同样也稍微解释下其中的代码，首先是位图的填充要按照先行后列的顺序，行对就宽，列对应高；其次反正切函数的值域是(−π, π]，所以要把它们映射到[0, 2π]；然后我们根据角度对应的区间，选择起始和终点颜色，再由线性插值得到各自的颜色分量，实践中每个颜色分量还乘上了当前的透明度，最后合成该点的颜色。</p>

<p><a href="https://github.com/DamianSheldon/QuartzDemo">完整示例</a></p>

<h2>Reference:</h2>

<p><a href="https://stackoverflow.com/questions/15344163/conical-gradient-in-qt-without-qconicalgradient">Conical gradient in Qt (without QConicalGradient)</a><br/>
<a href="https://github.com/maxkonovalov/MKGradientView">MKGradientView</a><br/>
<a href="https://en.wikipedia.org/wiki/Color_gradient">Color gradient</a><br/>
<a href="https://en.wikipedia.org/wiki/Linear_interpolation">Linear interpolation</a><br/>
<a href="https://en.wikipedia.org/wiki/Atan2">atan2</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS 开发问题汇总(十)]]></title>
    <link href="http://DamianSheldon.github.io/blog/ios-development-problems-part-10.html"/>
    <updated>2017-09-11T15:42:24+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/ios-development-problems-part-10</id>
    <content type="html"><![CDATA[<h3>1.在iOS 中如何使用私钥加密数据？</h3>

<p>A:当你用私钥加密时它被称为 siging, 之后用公钥解密的过程称为 verify signature.</p>

<blockquote><p>Why are you encrypting with the private key? When you encrypt with the private key, that is considered signing not encrypting, becuase it provides no confidentiality. If you want to &ldquo;encrypt&rdquo; with the private key, look into data signing, and that should allow you to &ldquo;encrypt&rdquo; (read &ldquo;sign&rdquo;) with the private key and &ldquo;decrypt&rdquo; (read &ldquo;verify signature&rdquo;) with the public key.</p></blockquote>

<p>Reference:<a href="https://stackoverflow.com/questions/6705928/encrypting-data-with-a-private-key-on-ios">Encrypting data with a private key on iOS</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS 中简单的图片处理]]></title>
    <link href="http://DamianSheldon.github.io/blog/simple-image-processing-in-ios.html"/>
    <updated>2017-07-14T09:49:40+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/simple-image-processing-in-ios</id>
    <content type="html"><![CDATA[<p>在 iOS 应用开发中，我们可能会要对图片进行旋转、缩放和裁剪，在介绍具体方法前，我们有必要先对图片做个大致的了解，这样有助于我们选择合适的方法。</p>

<h3>图片格式</h3>

<p>图片主要有两种格式：一种叫做位图；另一种称之为矢量图。所谓位图，就是把图片看成是由许多像素点组成；而矢量图则是用绘图指令来描述图片。举个例子，圆可以用圆点，半径，线条的粗细和颜色来描述它。从这个例子也可以看出用矢量图来描述风景，人物这样复杂的事物会比较复杂，所以它们通常会用位图来描述。</p>

<p>位图和矢量图也分为很多格式，具体可能查看<a href="https://en.wikipedia.org/wiki/Image_file_formats">Image file formats</a></p>

<p>下面我们讨论的是位图，在 iOS 中我们经常打交道的位图格式是 JPG 和 PNG，用来处理位图数据的类有：UIImage (UIKit)，CGImage (Core Graphics) 和 CIImage (Core Image)。Image I/O 本来是属于 Core Graphics，为了更加方便使用，Apple 将它分离出来成为单独的库。</p>

<h3>旋转</h3>

<p>既然位图是用一个一个的像素点来模拟图片，当我们想要旋转图片时，首先想到的方法自然是改变这些像素点的位置，这当然可以达到目标。要调整这么多像素点的位置自然要耗费不少时间，所有在数码相机刚出来那会，人们不是去改变像素点的位置，而是用一段数据来描述图片的方向等信息，这段数据称为 Exif. 所以对于 JPG 这种拥有 Exif 信息的位图，我们旋转图片的最佳做法自然是改变 Exif 里的方向信息。而 PNG 是没有 Exif 信息的，所以只能改变像素点的位置。</p>

<p>UIImage 自带了几个可以旋转的方法：</p>

<pre><code>+ (UIImage *)imageWithCGImage:(CGImageRef)cgImage scale:(CGFloat)scale orientation:(UIImageOrientation)orientation;

+ (UIImage *)imageWithCIImage:(CIImage *)ciImage scale:(CGFloat)scale orientation:(UIImageOrientation)orientation;

- (instancetype)initWithCGImage:(CGImageRef)cgImage scale:(CGFloat)scale orientation:(UIImageOrientation)orientation;

- (instancetype)initWithCIImage:(CIImage *)ciImage scale:(CGFloat)scale orientation:(UIImageOrientation)orientation;
</code></pre>

<!--more-->


<p>我们没有这个方法的实现源码，但是我们可以输出图片的 Exif 信息来验证上面的说法。iOS 中我们可以使用 Image I/O 这个库来读取和修改图片的 Exif 信息。Image I/O 的文档不是很详细，使用时最好结合头文件的说明，而且要注意区分容器和单个图片，实验表明像方向这种信息它并不是放在 Exif 中，而是图片属性中，它的值和 UIImageOrientation 也不是一一对应的，它们的关系如下：</p>

<pre><code>typedef NS_ENUM(NSInteger, DMLImagePropertyOrientation) {
    DMLImagePropertyOrientationUp               = 1,
    DMLImagePropertyOrientationDown             = 3,
    DMLImagePropertyOrientationLeft             = 8,
    DMLImagePropertyOrientationRight            = 6,
    DMLImagePropertyOrientationUpMirrored       = 2,
    DMLImagePropertyOrientationDownMirrored     = 4,
    DMLImagePropertyOrientationLeftMirrored     = 5,
    DMLImagePropertyOrientationRightMirrored    = 7
};

+ (DMLImagePropertyOrientation)dml_imagePropertyOrientationFromUIImageOrientation:(UIImageOrientation)imageOrientation
{
    DMLImagePropertyOrientation imagePropertyOrientation = DMLImagePropertyOrientationUp;

    switch (imageOrientation) {
        case UIImageOrientationUp:
            imagePropertyOrientation = DMLImagePropertyOrientationUp;
            break;

        case UIImageOrientationDown:
            imagePropertyOrientation = DMLImagePropertyOrientationDown;
            break;

        case UIImageOrientationLeft:
            imagePropertyOrientation = DMLImagePropertyOrientationLeft;
            break;

        case UIImageOrientationRight:
            imagePropertyOrientation = DMLImagePropertyOrientationRight;
            break;

        case UIImageOrientationUpMirrored:
            imagePropertyOrientation = DMLImagePropertyOrientationUpMirrored;
            break;

        case UIImageOrientationDownMirrored:
            imagePropertyOrientation = DMLImagePropertyOrientationDownMirrored;
            break;

        case UIImageOrientationLeftMirrored:
            imagePropertyOrientation = DMLImagePropertyOrientationLeftMirrored;
            break;

        case UIImageOrientationRightMirrored:
            imagePropertyOrientation = DMLImagePropertyOrientationRightMirrored;
            break;
    }

    return imagePropertyOrientation;
}
</code></pre>

<p>从打印输出的内容看出 JPG 图片的方向确实改变了，然后我也写了个方法去改变图片的方向属性，得到了同样的效果，所以当我们是旋转上面提到的方向直接使用 UIImage 自带的几个旋转的方法应该是最佳选择，而要旋转任意角度，还是要通过调整像素点位置来完成。</p>

<pre><code>// 输出图片的属性信息
- (UIImageOrientation)dml_imageOrientationFromExif
{
    UIImageOrientation imageOrientation = UIImageOrientationRightMirrored + 1;

    NSData *dataOfImage = UIImageJPEGRepresentation(self, (CGFloat)0.7);

    CGImageSourceRef imageSource = CGImageSourceCreateWithData((__bridge CFDataRef)dataOfImage, NULL);

    CFDictionaryRef imageProperties = CGImageSourceCopyPropertiesAtIndex(imageSource, 0, NULL);

    NSLog(@"dml_imageOrientationFromExif image Properties:%@\n", (__bridge NSDictionary *)imageProperties);

    CFRelease(imageSource);

    CFNumberRef numberOfImageOrientation = CFDictionaryGetValue(imageProperties, kCGImagePropertyOrientation);

    CFRelease(imageProperties);

    DMLImagePropertyOrientation imagePropertyOrientation = [(__bridge NSNumber *)numberOfImageOrientation integerValue];

    imageOrientation = [[self class] dml_uiimageOrientationFromImagePropertyOrientation:imagePropertyOrientation];

    return imageOrientation;
}

// 修改图片的方向属性
- (UIImage *)dml_setExifOritenation:(UIImageOrientation)imageOrientation error:(NSError * __autoreleasing *)error
{
    NSData *dataOfImage = UIImageJPEGRepresentation(self, (CGFloat)0.7);

    CGImageSourceRef imageSource = CGImageSourceCreateWithData((__bridge CFDataRef)dataOfImage, NULL);

    /* get the file type */
    CFStringRef UTI = CGImageSourceGetType(imageSource);
    if ( NULL == UTI ) {
        /* Handle Error Retrieving File Type Accordingly */
        if (error) {
            *error = [NSError errorWithDomain:(__bridge NSString *)kCFErrorDomainCGImageMetadata code:kCGImageMetadataErrorUnknown userInfo:@{NSLocalizedDescriptionKey: @"Handle Error Retrieving File Type Accordingly"}];
        }
        return nil;
    }

//    CFMutableDataRef finalImageData = (__bridge_retained CFMutableDataRef)dataOfImage.mutableCopy;
    CFMutableDataRef finalImageData = (__bridge_retained CFMutableDataRef)[NSMutableData new];

    /* create an image destination for saving the file */
    CGImageDestinationRef destination = CGImageDestinationCreateWithData(finalImageData, UTI, 1, NULL);
    if ( nil == destination ) {
        /* Handle Error Creating CGImageDestinationRef Accordingly */
        if (error) {
            *error = [NSError errorWithDomain:(__bridge NSString *)kCFErrorDomainCGImageMetadata code:kCGImageMetadataErrorUnknown userInfo:@{NSLocalizedDescriptionKey: @"Handle Error Creating CGImageDestinationRef Accordingly"}];
        }
        return nil;
    }

    /* setting properties */
//    CFDictionaryRef sourceProperties = CGImageSourceCopyProperties(imageSource, NULL);
    CFDictionaryRef sourceProperties = CGImageSourceCopyPropertiesAtIndex(imageSource, 0, NULL);

    NSLog(@"dml_setExifOritenation original properties:%@\n", (__bridge NSDictionary *)sourceProperties);

    CFMutableDictionaryRef mutableSourceProperties = CFDictionaryCreateMutableCopy(kCFAllocatorDefault, CFDictionaryGetCount(sourceProperties) + 1, sourceProperties);

    DMLImagePropertyOrientation imagePropertyOrientation = [[self class] dml_imagePropertyOrientationFromUIImageOrientation:imageOrientation];

    CFNumberRef numberForOritentation = CFNumberCreate(kCFAllocatorDefault, kCFNumberNSIntegerType, &amp;imagePropertyOrientation);

    CFDictionarySetValue(mutableSourceProperties, kCGImagePropertyOrientation, numberForOritentation);

    NSLog(@"dml_setExifOritenation edited properties:%@\n", (__bridge NSDictionary *)mutableSourceProperties);

    CGImageDestinationAddImageFromSource(destination, imageSource, 0, mutableSourceProperties);
    CGImageDestinationFinalize(destination);

    UIImage *resultImage = [UIImage imageWithData:(__bridge NSData *)finalImageData];

    CFRelease(numberForOritentation);

    CFRelease(mutableSourceProperties);

    CFRelease(sourceProperties);

    // Print destination properties

    NSData *dataOfDestinationImage = UIImageJPEGRepresentation(resultImage, (CGFloat)0.7);

    CGImageSourceRef destinationImageSource = CGImageSourceCreateWithData((__bridge CFDataRef)dataOfDestinationImage, NULL);

    CFDictionaryRef properties = CGImageSourceCopyPropertiesAtIndex(destinationImageSource, 0, NULL);

    NSLog(@"destination properties:%@\n", (__bridge NSDictionary *)properties);

    CFRelease(properties);
    CFRelease(destinationImageSource);

    return resultImage;
}
</code></pre>

<p>PS.理论上来讲，我们可以使用 <code>void CGImageDestinationSetProperties(CGImageDestinationRef idst, CFDictionaryRef properties);</code> 搭配 <code>bool CGImageDestinationCopyImageSource(CGImageDestinationRef idst, CGImageSourceRef isrc, CFDictionaryRef options, CFErrorRef  _Nullable *err);</code> 来改变图片的属性的，实际实验中并没有达到预期效果，原因不明。</p>

<h3>缩放</h3>

<p>上面提到 UIImage 的旋转方法也可以指定 Scale 因子，它是我们常说的几倍图中这个几倍因子，如果图片本来是一倍图，然后我们欺骗这个方法说是0.5倍图，那么我们会得到一张放大2倍的图，以些类推，所以我们可以考虑用这个方法来满足我们的一些简单需求，不能满足时，我们可以用 Core Graphics 将图片画到目标大小的位图上下文中来得到我们想要的图片。</p>

<h3>裁剪</h3>

<p>UIImage 没有裁剪相关的方法，我们可以使用 Core Graphics 中的 <code>CGImageRef CGImageCreateWithImageInRect(CGImageRef image, CGRect rect);</code>方法来实现裁剪。使用这个方法我们需要注意 Rect 要考虑图片的 scale，图片完整的 <code>rect = {0, 0, image.size.width * image.scale, image.size.height * image.scale}</code>, 所以我们指定裁剪的 rect 时也要带上 scale.</p>

<h3>缩略图</h3>

<p>生成缩略图可以使用 Image I/O 中的 <code>CGImageRef CGImageSourceCreateThumbnailAtIndex(CGImageSourceRef isrc, size_t index, CFDictionaryRef options);</code>方法。</p>

<h3>Reference</h3>

<ul>
<li>Image I/O Programming Guide</li>
<li><a href="https://objccn.io/issue-21-2/">图片格式</a></li>
<li><a href="http://nshipster.com/image-resizing/">Image Resizing Techniques</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[输出自定义尺寸视频]]></title>
    <link href="http://DamianSheldon.github.io/blog/how-to-specify-a-resolution-for-output-video.html"/>
    <updated>2017-04-10T10:12:09+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/how-to-specify-a-resolution-for-output-video</id>
    <content type="html"><![CDATA[<p>最近要做的一个项目中要拍摄视频，于是就开始来研究视频。看了几遍 AVFoundation Programming Guide 之后也写了个 Demo，把基本功能都过了一遍。这其中有意思的一件事情是我发现微信拍摄短视频的尺寸是 540x944, 这尺寸很奇怪，不是任何一个预设值。不清楚微信为什么用这么一个尺寸，但我想搞清楚怎么输出自定义尺寸的视频。</p>

<p>AVFoundation 捕获数据输出时，各组件的关系如下：</p>

<div style="text-align:center" markdown="1">
                                                                                           <img name="Capture Detail" src="http://DamianSheldon.github.io/images/captureDetail_2x.png">
                                                                                        </div>


<p>要想输出自定义尺寸的视频，我们可以从输入端和输出端着手。但是从文档来看，并没有提供可以自定捕获尺寸的方法，所以只能从输出端着手。</p>

<!--more-->


<p>首先我用 AVCaptureMovieFileOutput 做输出，然后调用 <code>setOutputSettings(_ outputSettings: [AnyHashable : Any]!, for connection: AVCaptureConnection!)</code> 来达到目标。但是很不幸，控制台输出了异常，查看 AVCaptureMovieFileOutput 的头文件，</p>

<blockquote><p>On iOS, you may only specify the AVVideoCodecKey in the outputSettings. If you specify any other key, an NSInvalidArgumentException will be thrown. See the availableVideoCodecTypes property.</p></blockquote>

<p>所以这个方法行不通。</p>

<p>于是我又尝试用 AVAssetWriter 来接收每一帧捕获的数据，然后按配置输出，理论上来讲这是可行的，实际上只有第一帧数据能成功被接收，之后的数据都会接收失败，具体原因不详。</p>

<p>直接处理每一帧数据失败之后，我又反复翻阅文档，发现编辑章节中提到可以修改 renderSize, 于是又一个想法冒出来，也许可以通过修改 renderSize 来输出自定义尺寸。按照文档编写好相关代码，激动地运行测试。结果得到的是：</p>

<pre><code>Optional(Error Domain=AVFoundationErrorDomain Code=-11800 "The operation could not be completed" UserInfo={NSUnderlyingError=0x1700505c0 {Error Domain=NSOSStatusErrorDomain Code=-12108 "(null)"}, NSLocalizedFailureReason=An unknown error occurred (-12108), NSLocalizedDescription=The operation could not be completed})
</code></pre>

<p>说实话，内心是崩溃的。但对这件事情还是耿耿于怀，又浏览了一下官方示例列表，发现了 AVSimpleEditoriOS,</p>

<blockquote><p>AVSimpleEditor is a simple AVFoundation based movie editing application which exercises the APIs of AVVideoComposition, AVAudioMix and demonstrates how they can be used for simple video editing tasks. It also demonstrates how they interact with playback (AVPlayerItem) and export (AVAssetExportSession). The application performs trim, rotate, crop, add music, add watermark and export. This sample is ARC-enabled.</p></blockquote>

<p>嗯，看到里面提到可以裁剪，于是就想它是怎么做？可以剪成我想要的大小吗？阅读相关的代码片断，原来它就是用的 renderSize 来实现裁剪的，跟我第三种方法的代码基本一致，差别是它是 Objc 写的，我用 Swift 写的。既然它能正常工作，那我就用这份代码来输出自定义尺寸吧。把代码移进来，运行测试，居然输出了指定的尺寸，难道代码用 Objc 和 Swift 写还有这种差别，整个人是懵的，这个原因暂时是不清楚的。</p>

<p>虽然输出的尺寸对了，但是画面没有铺满尺寸，而且方向错了。这有点太虐了，既然都走到这一步，就想那我再试试能不手动把它纠正吧。纠正的方法是使用 transform, 但是文档对它的介绍不详细，我先参考了 QuartZ 2D Programming Guide 中 Transforms 来变换，发现不对，整个画面全变成了黑色，又在 AVSimpleEditoriOS 的注释中发现了新的线索，</p>

<blockquote><p>Note: the point of origin for rotation is the upper left corner of the composition, t3 is to compensate for origin</p></blockquote>

<p>这么说它用的坐标和 QuartZ 2D 还不一样啊，这么坑爹，好吧，只能先确定好它们是怎么变换的。于是我先输出一段没变换的视频，之后每次测试一个变换，用这个办法确认了它们的变换是这样的，变换的原点是屏幕的左上角，Translation 向右是 X 轴的正方向，向下是 Y 轴的正方向; Rotation 的度数为正是按顺时针方向旋转，为负则是逆时针方向旋转；Scaling 的值大于1为放大，小于1则是缩小。</p>

<p>这样我就做了这么一个变换：</p>

<pre><code>t1 = CGAffineTransformScale(asset.preferredTransform, sx, sy);

t1 = CGAffineTransformRotate(t1, degreesToRadians(90));

t1 = CGAffineTransformTranslate(t1, 540, 0);
</code></pre>

<p>控制台报错了，说这个视频不支持编辑，这是个什么鬼？完全没有道理啊！于是我又把这段代码从下往上一行一行注释，看是谁导致的问题，发现是 <code>t1 = CGAffineTransformRotate(t1, degreesToRadians(90));</code> ，这样我又试着调整变换的顺序，改成：</p>

<pre><code>t1 = CGAffineTransformTranslate(asset.preferredTransform, 540, 0);

t1 = CGAffineTransformScale(t1, sx, sy);

t1 = CGAffineTransformRotate(t1, degreesToRadians(90));
</code></pre>

<p>运行测试，苍天啊，居然可以了。经历这么一出，感觉写代码都成了玄学了, 无力吐槽!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVFoundation 使用笔记]]></title>
    <link href="http://DamianSheldon.github.io/blog/notes-of-using-avfoundation.html"/>
    <updated>2017-04-06T15:09:30+08:00</updated>
    <id>http://DamianSheldon.github.io/blog/notes-of-using-avfoundation</id>
    <content type="html"><![CDATA[<p>使用一个框架时，我们可能有这么三个问题：</p>

<ol>
<li>这个框架是做什么的？</li>
<li>为什么要使用这个框架而不是其他的框架？</li>
<li>怎么用这个框架？</li>
</ol>


<h3>这个框架是做什么的？</h3>

<p>Apple 在 iOS Technology Overview 中的 Audio Technologies 和 Video Technologies 分别是这么介绍 AVFoundation 的：</p>

<blockquote><p>AV Foundation is an Objective-C interface for managing the recording and playback of audio and video. Use this framework for recording audio and when you need fine-grained control over the audio playback process.</p>

<p>AV Foundation provides advanced video playback and recording capabilities. Use this framework in situations where you need more control over the presentation or recording of video. For example, augmented reality apps could use this framework to layer live video content with other app-provided content.</p></blockquote>

<p>从这两个介绍中我们可以知道 AVFoundation 是用来播放和录制音频和视频的。</p>

<p>在 AVFoundation Programming Guide 中则是这么介绍的：</p>

<blockquote><p>AVFoundation is one of several frameworks that you can use to play and create time-based audiovisual media. It provides an Objective-C interface you use to work on a detailed level with time-based audiovisual data. For example, you can use it to examine, create, edit, or reencode media files. You can also get input streams from devices and manipulate video during realtime capture and playback.</p></blockquote>

<p>从这里我们可以知道它不仅可以播放和创建基于时间的视听媒体，还可以让我们在很细微的层面去操作这些视听数据。例如，你可以使用它检查、创建、编辑或者重编码媒体文件。你还可以用它从设备那里拿到输出流，并且可以在实时的捕获和播放过程中操作视频。</p>

<p>所以结论就是：这个框架是处理音频和视频的，而且处理的粒度可以非常细。</p>

<!--more-->


<h3>为什么要用这个框架而不是其他的框架？</h3>

<p>在选择框架时我们的原则应该首先是使用 Apple 自己提供的框架，其次才是第三方框架。在 Apple 自带的框架中选择时，又应该按抽象程度从高到低去选择。在音频技术中，抽象程度是这样的：Media Player framework > AVFoundation > OpenAL > Core Audio; 在视频技术中：UIImagePickerController > AVKit > AVFoundation > Core Media.</p>

<p>在音视频技术中，抽象程度高于 AVFoundation 的技术多侧重于简单的播放和录制，要进行其他的操作时则要使用 AVFoundation，而且它的能力也比较强，所以通常要对媒体数据进行处理时，我们会经常使用到它，它不满足要求时才去寻找其他的技术。</p>

<h3>怎么用这个框架？</h3>

<p>前面提到 AVFoundation 是用来播放、录制和操作视听数据的，操作视听数据则可以细分为 Editing 和 Exporting，所以我们这里会介绍这个框架:Playback, Capture, Editing 和 Exporting 四个大方面的使用。</p>

<h4>Playback</h4>

<p>在介绍怎么使用 AVFoundation 播放视听媒体之前，我们还要聊聊在 AVFoundation 中是怎么表示媒体的。AVFoundation 中用来代表媒体最主要的类是 AVAsset, 一个 AVAsset 实例是一片或多片媒体数据(音频曲目和视频轨迹)集合的综合代表。它提供关于这个集合的信息，例如它的标题，持续时间，本身的展示尺寸等等。AVAsset 没有绑定特定的数据类型。它是其他用来从指定 URL 媒体创建资产实例和创建新构成的父类。</p>

<p>资产中每片单独的媒体数据是统一的类型称为track. 在经典的简单场景，一个轨迹代表音频组件，另一个轨迹代表视频组件；在一个复杂的构成中，这里可能有多个重叠的音视频轨迹。</p>

<p>你为播放配置资产的方法取决于你想要播放资产的种类，广义上来讲，这里有两大类型：基于文件的资产和基于流的资产。</p>

<ol>
<li><p>加载和播放基于文件的资产。</p>

<ul>
<li>Create an asset using AVURLAsset.</li>
<li>Create an instance of AVPlayerItem using the asset.</li>
<li>Associate the item with an instance of AVPlayer.</li>
<li>Wait until the item’s status property indicates that it’s ready to play (typically you use key-value observing to receive a notification when the status changes).</li>
</ul>
</li>
<li><p>为播放创建和准备一个 HTTP 实时流。</p></li>
</ol>


<pre><code>NSURL *url = [NSURL URLWithString:@"&lt;#Live stream URL#&gt;];
// You may find a test stream at &lt;http://devimages.apple.com/iphone/samples/bipbop/bipbopall.m3u8&gt;.
self.playerItem = [AVPlayerItem playerItemWithURL:url];
[playerItem addObserver:self forKeyPath:@"status" options:0 context:&amp;ItemStatusContext];
self.player = [AVPlayer playerWithPlayerItem:playerItem];
</code></pre>

<ol>
<li><p>如果你不知道你拥有的 URL 是什么类型。</p>

<p> 1)Try to initialize an AVURLAsset using the URL, then load its tracks key.
 If the tracks load successfully, then you create a player item for the asset.</p>

<p> 2)If 1 fails, create an AVPlayerItem directly from the URL.
 Observe the player’s status property to determine whether it becomes playable.</p></li>
</ol>


<h4>Capture</h4>

<p>为了管理来自相机、麦克风的捕获，你组装对象去表示输入和输出，使用 AVCaptureSession 的实例来协调它们之间的数据流。你最少需要：</p>

<ul>
<li>一个 AVCaptureDevice 的实例来表示输入设备，例如相机或麦克风</li>
<li>一个 AVCaptureInput 具体子类的实例去配置来自输入设备的端口</li>
<li>一个 AVCaptureOutput 具体子类的实例去管理到电影或静态图片的输出</li>
<li>一个 AVCaptureSession 的实例来协调从输入到输出的数据流</li>
</ul>


<p>Capturing Video Frames as UIImage Objects</p>

<pre><code>// 1. Create and Configure a Capture Session
AVCaptureSession *session = [[AVCaptureSession alloc] init];
session.sessionPreset = AVCaptureSessionPresetMedium;

// 2. Create and Configure the Device and Device Input
AVCaptureDevice *device =
[AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];

NSError *error = nil;
AVCaptureDeviceInput *input =
[AVCaptureDeviceInput deviceInputWithDevice:device error:&amp;error];
if (!input) {
    // Handle the error appropriately.
}
[session addInput:input];

// 3. Create and Configure the Video Data Output
AVCaptureVideoDataOutput *output = [[AVCaptureVideoDataOutput alloc] init];
[session addOutput:output];
output.videoSettings =
@{ (NSString *)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) };
output.minFrameDuration = CMTimeMake(1, 15);

dispatch_queue_t queue = dispatch_queue_create("MyQueue", NULL);
[output setSampleBufferDelegate:self queue:queue];
dispatch_release(queue);

// 4. Implement the Sample Buffer Delegate Method
- (void)captureOutput:(AVCaptureOutput *)captureOutput
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
fromConnection:(AVCaptureConnection *)connection {

    UIImage *image = imageFromSampleBuffer(sampleBuffer);
    // Add your code here that uses the image.
}
// 5. Starting and Stopping Recording
NSString *mediaType = AVMediaTypeVideo;

[AVCaptureDevice requestAccessForMediaType:mediaType completionHandler:^(BOOL granted) {
    if (granted)
    {
        //Granted access to mediaType
        [self setDeviceAuthorized:YES];
    }
    else
    {
        //Not granted access to mediaType
        dispatch_async(dispatch_get_main_queue(), ^{
                [[[UIAlertView alloc] initWithTitle:@"AVCam!"
                message:@"AVCam doesn't have permission to use Camera, please change privacy settings"
                delegate:self
                cancelButtonTitle:@"OK"
                otherButtonTitles:nil] show];
                [self setDeviceAuthorized:NO];
                });
    }
}];

[session startRunning];

// To stop recording, you send the session a stopRunning message.
</code></pre>

<h4>Editing</h4>

<p>AVFoundation 框架提供了丰富的类来方便编辑音视资产。编辑 API 的核心是 composition. 一个 Compostion 是简单的来自一个或多个不同媒体资产的聚合。AVMutableCompostion 类提供插入和移除轨迹的接口，并且管理它们的时间顺序。图 3-1 展示了一个新的 composition
是如何用由现存资产联合形成的新资产拼装在一起的。如果你所有想要做的就是将多个资产按顺序的合成到一个单一的文件，那么这就是你得知道的所有细节。如果你想对你 compostion 里面的轨迹进行自定义的音频或视频处理，你相应地需要引入一个 audio mix 或 video compostion.</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableComposition" src="http://DamianSheldon.github.io/images/avmutablecomposition_2x.png">
</div>


<p>使用 AVMutableAudioMix 类， 你可以在你的 composition 的音频轨迹上进行自定义音频处理，像图 3-2 显示的。你现在可以指定一个最大的音量或者设置 volume ramp.</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableAudioMix" src="http://DamianSheldon.github.io/images/avmutableaudiomix_2x.png">
</div>


<p>你为了编辑可以像图 3-3 那样使用 AVMutableVideoCompostion 类来直接操作你 compostion 里的视频轨迹. 拥有一个 video composition, 你可以为输出视频指定想要的渲染尺寸,缩放以及帧率。通过一个 video composition&rsquo;s instruction(由 AVMutableVideoCompositionInstruction 类代表)，你可以修改你视频的背景颜色和应用 layer instructions. 这些 layer instructions（由 AVMutableVideoCompositionLayerInstruction 类代表) 可以用来应用 transforms, transform ramps, opacity and opacity ramps。Video
composition 类使用 animationTool 属性赋予你引入来自 Core Animation 框架效果的能力。</p>

<div style="text-align:center" markdown="1">
    <img name="AVMutableVideoCompostion" src="http://DamianSheldon.github.io/images/avmutablevideocomposition_2x.png">
</div>


<p>为了把你的 compostion 和 audio mix, video compostion 混合，你使用一个 AVAssetExportSession 对象，像 图 3-4 那样。你用你的 compostion 初始化 export session, 然后简单地把你的 audio mix 和 video composition 相应地赋值给 audioMix 和 videoComposition 属性。</p>

<div style="text-align:center" markdown="1">
    <img name="AVAssetExportSession" src="http://DamianSheldon.github.io/images/puttingitalltogether_2x.png">
</div>


<p>Combining Multiple Assets and Saving the Result to the Camera Roll</p>

<pre><code>// 1. Creating the Composition
AVMutableComposition *mutableComposition = [AVMutableComposition composition];
AVMutableCompositionTrack *videoCompositionTrack = [mutableComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
AVMutableCompositionTrack *audioCompositionTrack = [mutableComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];

// 2. Adding the Assets
AVAssetTrack *firstVideoAssetTrack = [[firstVideoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
AVAssetTrack *secondVideoAssetTrack = [[secondVideoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration) ofTrack:firstVideoAssetTrack atTime:kCMTimeZero error:nil];
[videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, secondVideoAssetTrack.timeRange.duration) ofTrack:secondVideoAssetTrack atTime:firstVideoAssetTrack.timeRange.duration error:nil];
[audioCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration)) ofTrack:[[audioAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0] atTime:kCMTimeZero error:nil];

// 3. Checking the Video Orientations
BOOL isFirstVideoPortrait = NO;
CGAffineTransform firstTransform = firstVideoAssetTrack.preferredTransform;
// Check the first video track's preferred transform to determine if it was recorded in portrait mode.
if (firstTransform.a == 0 &amp;&amp; firstTransform.d == 0 &amp;&amp; (firstTransform.b == 1.0 || firstTransform.b == -1.0) &amp;&amp; (firstTransform.c == 1.0 || firstTransform.c == -1.0)) {
        isFirstVideoPortrait = YES;
}
BOOL isSecondVideoPortrait = NO;
CGAffineTransform secondTransform = secondVideoAssetTrack.preferredTransform;
// Check the second video track's preferred transform to determine if it was recorded in portrait mode.
if (secondTransform.a == 0 &amp;&amp; secondTransform.d == 0 &amp;&amp; (secondTransform.b == 1.0 || secondTransform.b == -1.0) &amp;&amp; (secondTransform.c == 1.0 || secondTransform.c == -1.0)) {
        isSecondVideoPortrait = YES;
}
if ((isFirstVideoAssetPortrait &amp;&amp; !isSecondVideoAssetPortrait) || (!isFirstVideoAssetPortrait &amp;&amp; isSecondVideoAssetPortrait)) {
        UIAlertView *incompatibleVideoOrientationAlert = [[UIAlertView alloc] initWithTitle:@"Error!" message:@"Cannot combine a video shot in portrait mode with a video shot in landscape mode." delegate:self cancelButtonTitle:@"Dismiss" otherButtonTitles:nil];
            [incompatibleVideoOrientationAlert show];
                return;
}

// 4. Applying the Video Composition Layer Instructions
AVMutableVideoCompositionInstruction *firstVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set the time range of the first instruction to span the duration of the first video track.
firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);
AVMutableVideoCompositionInstruction * secondVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set the time range of the second instruction to span the duration of the second video track.
secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));
AVMutableVideoCompositionLayerInstruction *firstVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];
// Set the transform of the first layer instruction to the preferred transform of the first video track.
[firstVideoLayerInstruction setTransform:firstTransform atTime:kCMTimeZero];
AVMutableVideoCompositionLayerInstruction *secondVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoCompositionTrack];
// Set the transform of the second layer instruction to the preferred transform of the second video track.
[secondVideoLayerInstruction setTransform:secondTransform atTime:firstVideoAssetTrack.timeRange.duration];
firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];
secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];
AVMutableVideoComposition *mutableVideoComposition = [AVMutableVideoComposition videoComposition];
mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];

// 5. Setting the Render Size and Frame Duration
CGSize naturalSizeFirst, naturalSizeSecond;
// If the first video asset was shot in portrait mode, then so was the second one if we made it here.
if (isFirstVideoAssetPortrait) {
    // Invert the width and height for the video tracks to ensure that they display properly.
        naturalSizeFirst = CGSizeMake(firstVideoAssetTrack.naturalSize.height, firstVideoAssetTrack.naturalSize.width);
            naturalSizeSecond = CGSizeMake(secondVideoAssetTrack.naturalSize.height, secondVideoAssetTrack.naturalSize.width);
}
else {
    // If the videos weren't shot in portrait mode, we can just use their natural sizes.
        naturalSizeFirst = firstVideoAssetTrack.naturalSize;
            naturalSizeSecond = secondVideoAssetTrack.naturalSize;
}
float renderWidth, renderHeight;
// Set the renderWidth and renderHeight to the max of the two videos widths and heights.
if (naturalSizeFirst.width &gt; naturalSizeSecond.width) {
        renderWidth = naturalSizeFirst.width;
}
else {
        renderWidth = naturalSizeSecond.width;
}
if (naturalSizeFirst.height &gt; naturalSizeSecond.height) {
        renderHeight = naturalSizeFirst.height;
}
else {
        renderHeight = naturalSizeSecond.height;
}
mutableVideoComposition.renderSize = CGSizeMake(renderWidth, renderHeight);
// Set the frame duration to an appropriate value (i.e. 30 frames per second for video).
mutableVideoComposition.frameDuration = CMTimeMake(1,30);

// 6. Exporting the Composition and Saving it to the Camera Roll
// Create a static date formatter so we only have to initialize it once.
static NSDateFormatter *kDateFormatter;
if (!kDateFormatter) {
        kDateFormatter = [[NSDateFormatter alloc] init];
            kDateFormatter.dateStyle = NSDateFormatterMediumStyle;
                kDateFormatter.timeStyle = NSDateFormatterShortStyle;
}
// Create the export session with the composition and set the preset to the highest quality.
AVAssetExportSession *exporter = [[AVAssetExportSession alloc] initWithAsset:mutableComposition presetName:AVAssetExportPresetHighestQuality];
// Set the desired output URL for the file created by the export process.
exporter.outputURL = [[[[NSFileManager defaultManager] URLForDirectory:NSDocumentDirectory inDomain:NSUserDomainMask appropriateForURL:nil create:@YES error:nil] URLByAppendingPathComponent:[kDateFormatter stringFromDate:[NSDate date]]] URLByAppendingPathExtension:CFBridgingRelease(UTTypeCopyPreferredTagWithClass((CFStringRef)AVFileTypeQuickTimeMovie, kUTTagClassFilenameExtension))];
// Set the output file type to be a QuickTime movie.
exporter.outputFileType = AVFileTypeQuickTimeMovie;
exporter.shouldOptimizeForNetworkUse = YES;
exporter.videoComposition = mutableVideoComposition;
// Asynchronously export the composition to a video file and save this file to the camera roll once export completes.
[exporter exportAsynchronouslyWithCompletionHandler:^{
        dispatch_async(dispatch_get_main_queue(), ^{
                    if (exporter.status == AVAssetExportSessionStatusCompleted) {
                                    ALAssetsLibrary *assetsLibrary = [[ALAssetsLibrary alloc] init];
                                                if ([assetsLibrary videoAtPathIsCompatibleWithSavedPhotosAlbum:exporter.outputURL]) {
                                                                    [assetsLibrary writeVideoAtPathToSavedPhotosAlbum:exporter.outputURL completionBlock:NULL];
                                                                                }
                                                                                        }
                                                                                            });
}];
</code></pre>

<h4>Exporting</h4>

<p>为了读写视听资产，你必须使用 AVFoundation 框架提供的导出 API. AVAssetExportSession 类为简单的导出需求提供接口，例如修改文件格式或者裁剪资产的长度。对于更深的导出需求，使用 AVAssetReader 和 AVAssetWriter 类。</p>

<p>当你想要操作资产的内容时使用 AVAssetReader. 例如，你可能读取资产中的音频轨迹去生成表示声波的图形。使用 AVAssetWriter 从像 sample buffers 或 still images 的媒体中生成资产。</p>

<p>Reading an Asset</p>

<p>每个 AVAssetReader 对象一次只能关联单个的资产， 但是这个资产可能包含多个轨迹。基于这个原因，为了配置如何去读取媒体数据，你必须在读取前给你的 asset reader 赋予 AVAssetReaderOutput 的具体子类。这里有三个具体的子类：AVAssetReaderTrackOutput, AVAssetReaderAudioMixOutput 和 AVAssetReaderVideoCompositionOutput.</p>

<ol>
<li>Creating the Asset Reader</li>
</ol>


<pre><code>NSError *outError;
AVAsset *someAsset = &lt;#AVAsset that you want to read#&gt;;
AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:someAsset error:&amp;outError];
BOOL success = (assetReader != nil);
</code></pre>

<ol>
<li>Setting Up the Asset Reader Outputs</li>
</ol>


<pre><code>AVAsset *localAsset = assetReader.asset;
// Get the audio track to read.
AVAssetTrack *audioTrack = [[localAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
// Decompression settings for Linear PCM
NSDictionary *decompressionAudioSettings = @{ AVFormatIDKey : [NSNumber numberWithUnsignedInt:kAudioFormatLinearPCM] };
// Create the output with the audio track and decompression settings.
AVAssetReaderOutput *trackOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:audioTrack outputSettings:decompressionAudioSettings];
// Add the output to the reader if possible.
if ([assetReader canAddOutput:trackOutput])
        [assetReader addOutput:trackOutput];
</code></pre>

<ol>
<li>Reading the Asset’s Media Data</li>
</ol>


<pre><code>// Start the asset reader up.
[self.assetReader startReading];
BOOL done = NO;
while (!done)
{
    // Copy the next sample buffer from the reader output.
    CMSampleBufferRef sampleBuffer = [self.assetReaderOutput copyNextSampleBuffer];
    if (sampleBuffer)
    {
        // Do something with sampleBuffer here.
        CFRelease(sampleBuffer);
        sampleBuffer = NULL;
    }
    else
    {
        // Find out why the asset reader output couldn't copy another sample buffer.
        if (self.assetReader.status == AVAssetReaderStatusFailed)
        {
            NSError *failureError = self.assetReader.error;
            // Handle the error here.
        }
        else
        {
            // The asset reader output has read all of its samples.
            done = YES;
        }
    }
}
</code></pre>

<p>Writing an Asset</p>

<p>AVAssetWriter 类将多个来源的媒体数据按指定的文件格式写出到单一的文件。你不需要将你的 asset writer 对象和指定的资产关联起来，但是你必须为你想要创建的输出文件使用一个 asset writer. 因为一个 asset writer 可以写出来自多个源的媒体数据，你必须为你想要写出到输出文件的单独轨迹创建一个 AVAssetWriterInput 对象。每个 AVAssetWriterInput 对象期望收到 CMSampleBufferRef 对象格式的数据，但是如果你想追加 CVPixelBufferRef 对象到你的 asset writer input, 使用 AVAssetWriterInputPixelBufferAdaptor 类。</p>

<pre><code>// 1. Creating the Asset Writer
NSError *outError;
NSURL *outputURL = &lt;#NSURL object representing the URL where you want to save the video#&gt;;
AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:outputURL
fileType:AVFileTypeQuickTimeMovie
error:&amp;outError];
BOOL success = (assetWriter != nil);

// 2. Setting Up the Asset Writer Inputs
// Configure the channel layout as stereo.
AudioChannelLayout stereoChannelLayout = {
    .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,
    .mChannelBitmap = 0,
    .mNumberChannelDescriptions = 0
};

// Convert the channel layout object to an NSData object.
NSData *channelLayoutAsData = [NSData dataWithBytes:&amp;stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];

// Get the compression settings for 128 kbps AAC.
NSDictionary *compressionAudioSettings = @{
AVFormatIDKey         : [NSNumber numberWithUnsignedInt:kAudioFormatMPEG4AAC],
                        AVEncoderBitRateKey   : [NSNumber numberWithInteger:128000],
                        AVSampleRateKey       : [NSNumber numberWithInteger:44100],
                        AVChannelLayoutKey    : channelLayoutAsData,
                        AVNumberOfChannelsKey : [NSNumber numberWithUnsignedInteger:2]
};

// Create the asset writer input with the compression settings and specify the media type as audio.
AVAssetWriterInput *assetWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio outputSettings:compressionAudioSettings];
// Add the input to the writer if possible.
if ([assetWriter canAddInput:assetWriterInput])
    [assetWriter addInput:assetWriterInput];

// 3. Writing Media Data
// Prepare the asset writer for writing.
    [self.assetWriter startWriting];
    // Start a sample-writing session.
    [self.assetWriter startSessionAtSourceTime:kCMTimeZero];
    // Specify the block to execute when the asset writer is ready for media data and the queue to call it on.
    [self.assetWriterInput requestMediaDataWhenReadyOnQueue:myInputSerialQueue usingBlock:^{
        while ([self.assetWriterInput isReadyForMoreMediaData])
        {
            // Get the next sample buffer.
            CMSampleBufferRef nextSampleBuffer = [self copyNextSampleBufferToWrite];
            if (nextSampleBuffer)
            {
                // If it exists, append the next sample buffer to the output file.
                [self.assetWriterInput appendSampleBuffer:nextSampleBuffer];
                CFRelease(nextSampleBuffer);
                nextSampleBuffer = nil;
            }
            else
            {
                // Assume that lack of a next sample buffer means the sample buffer source is out of samples and mark the input as finished.
                [self.assetWriterInput markAsFinished];
                break;
            }
        }
    }];
</code></pre>

<p>Reference:<br/>
iOS Technology Overview<br/>
AVFoundation Programming Guide</p>
]]></content>
  </entry>
  
</feed>
